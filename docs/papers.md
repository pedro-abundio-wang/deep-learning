---
layout: default
comments: false
keywords:

title: Papers
description:
buttons:
micro_nav: false
---

## Papers

<table id="schedule" class="table table-bordered no-more-tables" style="width: 100%; font-size: 0.8em;">
    <colgroup>
        <col style="width: 70%;">
        <col style="width: 30%;">
    </colgroup>
    <thead class="active" style="background-color:#f9f9f9" align="left">
        <th>Paper</th>
        <th>Description</th>
    </thead>
    <tbody>
        <tr>
            <td id="Optimization" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Optimization</strong>
            </td>
        </tr>
        <tr>
            <td>
				<a href="https://arxiv.org/abs/1206.5533">
					Practical Recommendations for Gradient-Based Training of Deep Architectures
				</a>
			</td>
            <td>
				
			</td>
        </tr>
		<tr>
            <td>
				<a href="https://arxiv.org/abs/1803.05407">
					Averaging Weights Leads to Wider Optima and Better Generalization
				</a>
			</td>
            <td></td>
        </tr>
		<tr>
            <td>
				<a href="https://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf">
					Train longer generalize better
				</a>				
			</td>
            <td></td>
        </tr>
		<tr>
            <td>
				<a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">
					Random Search for Hyper-Parameter Optimization
				</a>
			</td>
            <td>
				<ul>
					<li>Random HyperParameter Search</li>
					<li>Grid HyperParameter Search</li>
				</ul>
			</td>
        </tr>
        <tr>
            <td id="BatchSize" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>BatchSize</strong>
            </td>
        </tr>
        <tr>
            <td>
			    <a href="https://arxiv.org/pdf/1609.04836.pdf">
					On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima
				</a>
			</td>
            <td></td>
        </tr>
		<tr>
            <td>
			    <a href="https://arxiv.org/pdf/1706.02677.pdf">
					Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour
				</a>
			</td>
            <td></td>
        </tr>
		<tr>
            <td>
			    <a href="https://arxiv.org/pdf/1711.00489.pdf">
					Don't Decay the Learning Rate, Increase the Batch Size
				</a>
			</td>
            <td></td>
        </tr>
        <tr>
            <td id="Gradient Algorithms" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Gradient Algorithms</strong>
            </td>
        </tr>
        <tr>
            <td>
				<a href="https://arxiv.org/abs/1502.04390">
					Equilibrated adaptive learning rates for non-convex optimization
				</a>
			</td>
            <td>RMSProp</td>
        </tr>
		<tr>
            <td>
				<a href="https://arxiv.org/abs/1412.6980">
					A METHOD FOR STOCHASTIC OPTIMIZATION
				</a>
			</td>
            <td>ADAM</td>
        </tr>
		<tr>
            <td>
				<a href="https://arxiv.org/pdf/1712.07628.pdf">
					Improving Generalization Performance by Switching from Adam to SGD
				</a>
			</td>
            <td></td>
        </tr>
        <tr>
            <td id="Initialization" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Initialization</strong>
            </td>
        </tr>
        <tr>
            <td>
				<a href="http://proceedings.mlr.press/v9/glorot10a.html">
					Understanding the difficulty of training deep feedforward neural networks
				</a>
			</td>
            <td>Xavier Initialization</td>
        </tr>
		<tr>
            <td>
				<a href="https://arxiv.org/pdf/1502.01852.pdf">
					Delving Deep into Rectifiers
				</a>
			</td>
            <td>He Initialization</td>
        </tr>
        <tr>
            <td id="Regularization" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Regularization</strong>
            </td>
        </tr>
        <tr>
            <td>
				<a href="https://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf">
					a simple weight decay can improve generalization
				</a>
			</td>
            <td>Regularization</td>
        </tr>
		<tr>
            <td>
				<a href="http://jmlr.org/papers/v15/srivastava14a.html">
					A Simple Way to Prevent Neural Networks from Overfitting
				</a>
			</td>
            <td>Dropout</td>
        </tr>
		<tr>
            <td>
				<a href="https://arxiv.org/pdf/1802.01223.pdf">
					Learning Compact Neural Networks with Regularization
				</a>
			</td>
            <td></td>
        </tr>
		<tr>
            <td id="Normalization" colspan="2" style="text-align:center; vertical-align:middle;background-color:#b7ffbf">
                <strong>Normalization</strong>
            </td>
        </tr>
        <tr>
            <td>
				<a href="https://arxiv.org/abs/1502.03167">
					Accelerating Deep Network Training by Reducing Internal Covariate Shift
				</a>
			</td>
            <td>Batch Normalization</td>
        </tr>
    </tbody>
</table>

