<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Section 3 (Week 3)</title>

	<meta name="description" content="Understanding gradient descent and backpropagation. We will go through four different neural network examples and explicitly compute the backpropagation equations.">


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/papers">Papers</a></li>
			
				<li><a href="/projects">Projects</a></li>
			
				<li><a href="/sections">Sections</a></li>
			
				<li><a href="/lectures">Lectures</a></li>
			
				<li><a href="/blogs">Blogs</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							Deep Learning
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/papers">Papers</a></li>
							
								<li><a href="/projects">Projects</a></li>
							
								<li><a href="/sections">Sections</a></li>
							
								<li><a href="/lectures">Lectures</a></li>
							
								<li><a href="/blogs">Blogs</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Section 3 (Week 3)</h1>
								
								
									<p class="hero-subheader__desc">Understanding gradient descent and backpropagation. We will go through four different neural network examples and explicitly compute the backpropagation equations.</p>
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<h1 id="introduction">Introduction</h1>

<p>When datasets are large and high-dimensional, it is computationally very expensive (sometimes impossible!) to find an analytical solution for the optimal parameters of your network. Instead, we use optimization methods. A vanilla optimization approach would be to sample different combinations of parameters and choose the one with the lowest loss value.</p>
<ul>
  <li><strong>Is this a good idea?</strong></li>
  <li><strong>Would it be possible to extract another piece of information to direct our search towards the optimal parameters?</strong></li>
</ul>

<table class="image">
<caption align="bottom"><small>The trajectory through a loss landscape for a linear regression model trained with gradient descent. The red dot indicates the value of the loss function corresponding to the initial parameter values. (<a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/optimization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/3/optimization.png" style="" /></td></tr>
</table>

<p>This is exactly what <em>gradient descent</em> does!  Apart from the loss value, gradient descent computes the local gradient of the loss when evaluating potential parameters. This information is used to decide which direction the search should go to find better parameter values. This extra piece of information (the local gradient) can be computed relatively easily using <em>backpropagation</em>. This recursive algorithm breaks up complex derivatives into smaller parts through the <em>chain rule</em>.</p>

<p>To help understand gradient descent, let’s <strong><a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/optimization/index.html">visualize</a></strong> the setup.</p>

<h1 id="univariate-regression">Univariate Regression</h1>
<p>Let’s consider a linear regression. You have a data set <script type="math/tex">(x,y)</script> with <script type="math/tex">m</script> examples. In other words, <script type="math/tex">x = (x_1,\dots , x_m)</script> and <script type="math/tex">y = (y_1, \dots, y_m)</script> are row vectors of <script type="math/tex">m</script> scalar examples . The goal is to find the scalar parameters <script type="math/tex">w</script> and <script type="math/tex">b</script> such that the line <script type="math/tex">y = wx + b</script> optimally fits the data. This can be achieved using gradient descent.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/3/1/model.png" style="" /></td></tr>
</table>

<h3 id="forward-propagation">Forward Propagation</h3>
<p>The first step of gradient descent is to compute the loss. To do this, define your model’s output and loss function. In this regression setting, we use the mean squared error loss.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\hat{y} &= wx + b\\\\
	\mathcal{L} &= \frac{1}{m}||\hat{y} - y||^2
\end{align} %]]></script>

<h3 id="backward-propagation">Backward Propagation</h3>
<p>The next step is to compute the local gradient of the loss with respect to the parameters (i.e. <script type="math/tex">w</script> and <script type="math/tex">b</script>). This means you need to calculate derivatives. Note that values stored during the forward propagation are used in the gradient equations.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{\partial\mathcal{L}}{\partial w} &= \frac{2}{m}(\hat{y} - y)x^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b} &= \frac{2}{m}(\hat{y} - y)\vec{1}
\end{align} %]]></script>

<h1 id="multivariate-regression">Multivariate Regression</h1>

<p>Now, consider the case where <script type="math/tex">X</script> is a matrix of shape <script type="math/tex">(n,m)</script> and <script type="math/tex">y</script> is still a row vector of shape <script type="math/tex">(1,m)</script>. Instead of a single scalar value, the weights will be a vector (one element per feature) of shape <script type="math/tex">(1,n)</script>. The bias parameter is still a scalar.</p>
<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/3/2/model.png" style="" /></td></tr>
</table>

<h3 id="forward-propagation-1">Forward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\hat{y} &= wX + b\\\\
	\mathcal{L} &= \frac{1}{m}||\hat{y} - y||^2
\end{align} %]]></script>

<h3 id="backward-propagation-1">Backward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{\partial\mathcal{L}}{\partial w} &= \frac{2}{m}(\hat{y} - y)X^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b} &= \frac{2}{m}(\hat{y} - y)\vec{1}
\end{align} %]]></script>

<h1 id="two-layer-linear-network">Two Layer Linear Network</h1>
<p>Consider stacking two linear layers together. You can introduce a hidden variable <script type="math/tex">Z</script> of shape <script type="math/tex">(k, m)</script>, which is the output of the first linear layer. The first layer is parameterized by a weight matrix <script type="math/tex">W_1</script> of shape <script type="math/tex">(k,n)</script> and bias <script type="math/tex">b_1</script> of shape <script type="math/tex">(k, 1)</script> broadcasted to <script type="math/tex">(k, m)</script>. The second layer will be the same as in the multivariate regression case, but its input will be <script type="math/tex">Z</script> instead of <script type="math/tex">X</script>.</p>
<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/3/3/model.png" style="" /></td></tr>
</table>

<h3 id="forward-propagation-2">Forward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	Z &= W_1 X + b_1\\\\
	\hat{y} &= w_2Z + b_2\\\\
	\mathcal{L} &= \frac{1}{m}||\hat{y} - y||^2
\end{align} %]]></script>

<h3 id="backward-propagation-2">Backward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{\partial\mathcal{L}}{\partial W_1} &= w_2^\intercal\frac{2}{m}(\hat{y} - y)X^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b_1} &= w_2^\intercal\frac{2}{m}(\hat{y} - y)\vec{1}\\\\
	\frac{\partial\mathcal{L}}{\partial w_2} &= \frac{2}{m}(\hat{y} - y)Z^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b_2} &= \frac{2}{m}(\hat{y} - y)\vec{1}
\end{align} %]]></script>

<h1 id="two-layer-nonlinear-network">Two Layer Nonlinear Network</h1>
<p>In this example, before sending <script type="math/tex">Z</script> as the input to the second layer, you will pass it through the sigmoid function. The output is denoted <script type="math/tex">A</script> and is the input of the second layer.</p>
<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/3/4/model.png" style="" /></td></tr>
</table>

<h3 id="forward-propagation-3">Forward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	Z &= W_1 X + b_1\\\\
	A &= \sigma(Z)\\\\
	\hat{y} &= w_2A + b_2\\\\
	\mathcal{L} &= \frac{1}{m}||\hat{y} - y||^2
\end{align} %]]></script>

<h3 id="backward-propagation-3">Backward Propagation</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{\partial\mathcal{L}}{\partial W_1} &= \left(\left(w_2^\intercal\frac{2}{m}(\hat{y} - y)\right)\odot A \odot (1 - A)\right)X^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b_1} &= \left(\left(w_2^\intercal\frac{2}{m}(\hat{y} - y)\right)\odot A \odot (1 - A)\right)\vec{1}\\\\
	\frac{\partial\mathcal{L}}{\partial w_2} &= \frac{2}{m}(\hat{y} - y)A^\intercal\\\\
	\frac{\partial\mathcal{L}}{\partial b_2} &= \frac{2}{m}(\hat{y} - y)\vec{1}
\end{align} %]]></script>

<h1 id="helpful-resources">Helpful Resources</h1>

<ul>
  <li><a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf">Matrix derivatives “cheat” sheet</a></li>
  <li><a href="http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf">CS229 Lecture Notes</a></li>
  <li><a href="http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf">CS229 Backpropagation</a></li>
</ul>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2019. - Pedro Abundio Wang <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
