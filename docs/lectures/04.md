---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Generative Adversarial Networks
description:

# Micro navigation
micro_nav: true

---

Neural networks are vulnerable. Online platforms run powerful neural networks-based classifiers to flag violent or sexual content for removal. A malicious attacker can circumvent these classifiers by introducing imperceptible perturbations on these images, allowing them to live on the platform! This is called an Adversarial Attack.

## Attacking Neural Networks with Adversarial Examples

### Motivation

In 2013, [Szegedy et. al](https://arxiv.org/pdf/1312.6199.pdf) identified that it is possible to create a fake datapoint [image](https://arxiv.org/pdf/1312.6199.pdf), [text](https://arxiv.org/pdf/1707.02812.pdf), [text](http://www.aclweb.org/anthology/P18-2006),  [speech](https://arxiv.org/pdf/1801.01944.pdf) or even [structured data](http://patrickmcdaniel.org/pubs/esorics17.pdf) to fool a classifier. In other words, you can generate a fake image that the neural network classifies as a target class you have chosen, for example a cat.

{% include image.html description="" link="" image="lecture/4/fake_image.png" caption="false"%}

The generated images might even look real to humans. For instance, an impercetible perturbation on a cat image fools a model to classify it as a cat, while the image still looks like a cat to human.

{% include image.html description="" link="" image="lecture/4/cat_to_iguana.png" caption="false"%}

These observations have deep consequences on productization neural networks. Here are some examples to be aware of:

- Autonomous cars use object detectors to localize multiple objects including pedestrians, cars and traffic signs. An imperceptible perturbation of the 'STOP' sign image processed by the object detector could  a '70 miles' speed limit. This could lead to serious trouble in a real-life setting.
- Face identification systems screen individuals to allow or refuse entrance to a building. A forged image perturbation of an unauthorized person's face could make this person be authorized.
- Websites run powerful classifiers to detect sexual and violent content online. Imperceptible perturbations of prohibited images could bypass these classifiers and lead to false negative predictions.

We will first present adversarial attacks, then delve into defenses.

### Adversarial Attacks

#### General procedure

Consider a model pre-trained on [ImageNet](http://www.image-net.org/papers/imagenet_cvpr09.pdf). The following framework is a quick way to forge an example which will be classified as $$i$$.

1. Pick a sample $$x$$ and forward propagate it in the model to get prediction $$\hat{y}$$.
2. Define a loss function that quantifies the distance between $$\hat{y}$$ and the target class prediction $$y_j$$. For example: 
$$\mathcal{L}(\hat{y}, y_j) = \frac{1}{2}||\hat{y} - y_i||^2$$
3. While keeping the parameters of the neural network frozen, minimize the defined loss function by updating the pixels of $$x$$ iteratively via gradient descent ($$x = x - \alpha \frac{\partial \mathcal{L}}{\partial x}$$). You will find $$x_{adv}$$ that is classified as class $$y_i$$ with high probability.

Because the only constrain in the loss function involved the output prediction, the forged image might be any image from the input space. In fact, the solution of the optimization problem above is much more likely to look random than real. Recall that the size of the space of 32x32 colored images is $$255^{(32 \times 32 \times 3)} ≈ 10^{7400}$$ which is much larger than the space of 32x32 colored real images.

However, you can alleviate that issue by adding a term to the loss that forces the image $$x$$ to be close to a chosen image $$x_j$$. This is an example of a relevant loss function:

$$\mathcal{L}(\hat{y}, y_j, x) = \frac{1}{2}||\hat{y} - y_i||^2 + \lambda \cdot ||x - x_j||^2$$

where you can tune hyperparameter $$\lambda$$ to balance the trade-off between the two terms of the loss expression.

After running the optimization process with this procedure, you find an adversarial example that looks like $$x_j$$ but is classified as $$y_i$$.

#### In practice

Using additional tricks such as gradient clipping, you can ensure that the differencec between the generated image and the target image ($$x_j$$) are imperceptible.

### Defenses to adversarial attacks

#### Types of attack

There are two types of attacks:
- white-box: The attacker has access to information about the network.
- black-box: The attacker does not have access to the network, but can query it (i.e. send inputs and observe predictions.)

In the optimization above, computing $$\frac{\partial \mathcal{L}}{\partial x}$$ using backpropagation requires access to the network weights. However, in a black-box setting, you could approximate the gradient by making tiny changes to the input and observing the output ($$\frac{\partial \mathcal{L}}{\partial x} ≈ \frac{f(x+\varepsilon) - f(x)}{(x + \varepsilon) - x}$$)

#### Defense methods

Defenses against adversarial attacks is an important research topic. Although no method have been proven to counter all attacks yet, the following directions have been proposed:

1. Create a [SafetyNet](https://arxiv.org/pdf/1704.00103.pdf) acting as a FireWall to stop adversarial examples from fooling your model. SafetyNet should be hard to optimized such that it is difficult to produce examples that are both misclassified and slip past SafetyNet’s detector.
2. Add correctly labelled adversarial examples to the training set. This consists of simply training the initial network with additional examples to force the representation of each class to take into account small perturbations.
3. Use Adversarial training. For each example fed to your network, the loss also takes into account the prediction of a perturbation $$x_{adv}$$ of the input $$x$$ of label $$y$$. In other words, by calling $$\mathcal{L}$$ the 'normal' loss of the network, $$W$$ its parameters, a possible candidate for the new loss can be given by $$L(W,x,y) + \lambda L(W,x_{adv}, y)$$. Here, $$\lambda$$ is a hyperparameter that balances model robustness versus performance towards real data.

Note that (2) and (3) are computationally expensive using the iterative method of generating adversarial example. In the next part, you will learn a method called Fast Gradient Sign Method (FGSM) which generates adversarial examples in one pass.

### Why are neural networks vulnerable to adversarial examples?

#### Adversarial attack on logistic regression

Consider a logistic regression defined by: $$\hat{y} = \sigma{Wx + b}$$, where $$x \mapsto \sigma(x)$$ indicates the sigmoid function. The shapes are: $$x \in \mathbb{R}^{n \times 1}$$, $$W \in \mathbb{R}^{1 \times n}$$ and $$\hat{y} \in \mathbb{R}^{1 \times 1}$$. For simplicity, assume $$b = 0$$ and $$n = 6$$.

Let $$W = (1, 3, -1, 2, 2, 3)$$ and $$x = (1, -1, 2, 0, 3, -2)^T$$. In this setting, $$\hat{y} = 0.27$$. This means that with $$73\%$$ probability, the predicted class is $$y = 0$$.

Now, how can you change $$x$$ slightly and convert the model's prediction to $$y = 1$$? One way is to move components of $$x$$ in the same direction as $$sign(W)$$. By doing this, you ensure that the perturbations will lead to a positive addition to $$\hat{y}$$.

$$\begin{align} 
x_{adv} &= x + \varepsilon sign(W) \\
&= (1, -1, 2, 0, 3, -2)^T + (0.4, 0.4, -0.4, 0.4, 0.4, 0.4) \\
&= (1.4, -0.6, 1.6, 0.4, 3.4, -1.6) \\
\end{align}$$ 

For $$\epsilon = 0.4$$ leads to 

$$\begin{align} 
\hat{y} &= \sigma{Wx_{adv} + b} \\
&= \sigma (1, 3, -1, 2, 2, 3) \cdot (1.4, -0.6, 1.6, 0.4, 3.4, -1.6) \\
&= \sigma(1.4 - 1.8 - 1.6 + 0.8 + 6.8 - 4.8) \\
&= \sigma(0.8) \\
&= 0.69 \\
\end{align}$$ 

The prediction is now $$y = 1$$ with 69% confidence.

This small example illustrates that a rightly chosen $$\epsilon$$ perturbation can have a considerable impact on the output of a neural network.

#### Comments

- Despite being less powerful, using $$\varepsilon sign(W)$$ instead of $$\varepsilon W$$ ensures that $$x_{adv}$$ stays close to $$x$$. The perturbation is indeed capped by $$\varepsilon$$.
- The larger $$n$$, the more powerful the attack. Indeed, perturbating each feature of $$x$$ additively impacts $$\hat{y}$$.
- For neural networks, $$x_{adv} = x + \varepsilon sign(W)$$ can be generalized to $$x_{adv} = x + \varepsilon sign(\nabla_x \mathcal{L}(W,x,y))$$. The intuition is that you want to push $$x$$ with limited amplitude in the direction of positive changes of the loss function. This method is called the **Fast Gradient Sign Method (FGSM)**. It generates adversarial examples much faster than the iterative method described earlier.

#### Conclusion

Although early attempts at explaining the existence of adversarial examples focused on overfitting and network non-linearity, Goodfellow et al. (in Explaining and Harnessing Adversarial Examples) found that it is due to the linearity of the network. Indeed, neural networks have been designed to behave linearly (think about the introduction of ReLU, LSTMs, etc.) in order to ease to optimization process. Even activation functions such as $$\sigma$$ and $$tanh$$ have been designed to function in their linear regime with methods such as Xavier Initialization and BatchNorm. However, the FGSM illustrates that easy-to-train models are easy-to-attack because the gradients can backpropagate.

#### Further readings

- If you're looking for a survey of adversarial examples, [Yuan et al.](https://arxiv.org/pdf/1712.07107.pdf) offer a review of recent findings on adversarial examples for deep neural networks.
- If you are more interested in adversarial attack and defense scenarios, [Kurakin et al.](https://arxiv.org/pdf/1804.00097.pdf) is a great supplement.

## Generative Adversarial Networks

Neural networks are widely used to predict. What if they could be used to generate new images, texts or even audio clips?

Imagine training a robotic arm to localize objects on a table (in order to grasp them.) Collecting real data for this task is expensive. It requires to position objects on a table, take pictures and label with bounding boxes. Alternatively, taking screenshots in simulations allows you to virtually generate millions of labelled images. The downside is that a network trained on simulated data might not generalize to real data. Having a network that generates real homologues of simulated images would be a game changer. This is one example of application of Generative Adversarial Networks (GANs.)

We will give you a thorough grounding on GANs and how to apply them to cutting-edge tasks.

### Motivation

Are networks capable of generating images of cats they have never seen? Intuitively, they should be. If a cat vs. non-cat classifier generalizes to unseen data, it means that it understands the salient features of the data (i.e. what a cat is and isn't) instead of overfitting the training data. Similarly, a generative model should be able to generate pictures of cats it has never seen because its complexity (~ number of parameters) doesn't allow it to memorize the training set.

For instance, the following cats, cars and faces were generated by [Karras et al.](http://www.image-net.org/papers/imagenet_cvpr09.pdf) using GANs. They do not exist in reality!

{% include image.html description="" link="" image="lecture/4/fake_cats.png" caption="false"%}
{% include image.html description="" link="" image="lecture/4/fake_cars.png" caption="false"%}
{% include image.html description="" link="" image="lecture/4/fake_faces.png" caption="false"%}

### The generator vs. discriminator game

Although there exist various generative algorithms, this article will focus on the study of GANs.

A [GAN](https://arxiv.org/abs/1406.2661) involves two neural networks. The first network is called the "generator" ($$G$$) and its goal is to generate realistic samples. The second network is a binary classifier called the "discriminator", and its goal is to differentiate fake samples (label $$0$$) from real sample (label $$1$$.)

These two networks play a game. $$D$$ alternatively receives real samples from a database and fake samples generated by $$G$$, and has to learn to differentiate them. At the same time, $$G$$ learns to fool $$D$$. The game ends when $$G$$ generates samples that are realistic enough to fool $$D$$. When training ends successfully, you can use $$G$$ to generate realistic samples. Here's an illustration of the GAN game.

{% include image.html description="" link="" image="lecture/4/GAN_game.png" caption="false"%}

It is common to choose a random code $$z$$ as input to $$G$$, such that $$x = G(z)$$ is a generated image. Later, you will learn alternative designs for z allowing you to choose the features of $$x$$. 

### Training GANs

To training the GAN, you need to optimize two cost function simultaneously.

- Discrimator cost $$J^{(D)}$$: $$D$$ is a binary classifier aiming to map inputs $$x=G(z)$$ to $$y = 0$$ and inputs $$x=x_{real}$$ to $$y = 1$$. Thus, the logistic loss (a.k.a. binary cross-entropy) is appropriate:

$$J^{(D)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} y_{\text{real}}^{(i)}\log (D(x^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} (1-y_{\text{gen}}^{(i)})\log (1-D(G(z^{(i)})))$$

where $$m_{\text{real}}$$ (resp. $$m_{\text{gen}}$$) is the number of real (resp. generated) examples in a batch. $$y_{\text{gen}} = 0$$ and $$y_{\text{real}} = 1$$.

- Generator cost $$J^{(G)}$$: Since success is measured by the ability of $$G$$ to fool $$D$$, $$J^{(G)}$$ should be the opposite of $$J^{(D)}$$:

$$J^{(G)} = \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D(G(z^{(i)})))$$

Note: the first term of $$J^{(D)}$$ does not appear in $$J^{(G)}$$ because it is independent of $$z$$ and will entail no gradient during optimization.

You can run an optimization algorithm such as [Adam](https://arxiv.org/abs/1412.6980) simultaneously using two mini-batches of real and fake samples. You can think of it as a two step process: 
1. Forward propagate a mini-batch of real samples, compute $$J^{(D)}$$. Then, backprogate to compute $$\frac{\partial J^{(D)}}{\partial W_D}$$ where $$W_D$$ denotes the parameters of $$D$$.
2. Forward propagate a mini-batch of samples freshly generated by $$G$$, compute $$J^{(D)}$$ and $$J^{(G)}$$. Then backpropagate to compute $$\frac{\partial J^{(D)}}{\partial W_D}$$ and $$\frac{\partial J^{(G)}}{\partial W_G}$$ where $$W_G$$ denotes the parameters of $$G$$.

{% include image.html description="" link="" image="lecture/4/GAN_game_training.png" caption="false"%}

If training is successful, the distribution of fake samples coming from $$G$$ should match the true distribution of data.

In mathematical terms, convergence is guaranteed. Here's why....

This [repository](https://github.com/eriklindernoren/Keras-GAN/blob/master/gan/gan.py) is a nice code example on how to train a GAN.

### Tips to train GANs

In practice, training GANs is hard and requires subtle tricks.

#### Using a non-saturating cost

GAN training is an iterative game between $$D$$ and $$G$$. If $$G(z)$$ isn't realistic, $$D$$ doesn't need to improve. Alternatively, if $$D$$ is easy to fool, $$G$$ doesn't need to improve the realism of the generated samples. Consequently, $$D$$ and $$G$$ need to grow together in quality.

Early in the training, $$G$$ is usually generating random noise and $$D$$ easily differentiates $$x=G(z)$$ from real samples. This unbalanced power hinders training. To understand why, let's plot $$J^{(G)}$$ against $$D(G(z))$$:

{% include image.html description="" link="" image="lecture/4/sat_cost_alone.png" caption="false"%}

On the graph above, consider the x-axis to be $$D(G(z))$$ "in expectation" over a given batch of examples. 

The gradient $$\frac{\partial J^{(G)}}{\partial D(G(z))}$$ represented by the slope of the plotted function is small early in the training (i.e. when $$D(G(z))$$ is close to $$0$$.) As a consequence, the backpropagated gradient $$\frac{\partial J^{(G)}}{\partial W_G}$$ is also small. Fortunately, the following simple mathematical trick solves the problem:

$$\begin{align} 
min (J^{(G)}) &= \min \Big[ \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D(G(z^{(i)}))) \Big] \\
&= \max \Big[ \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D(G(z^{(i)}))) \Big] \\
&= \min \Big[ - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D(G(z^{(i)}))) \Big]
\end{align}$$ 

As you can see on the graph below, minimizing $$ - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D(G(z^{(i)})))$$ instead of $$ \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D(G(z^{(i)})))$$ ensures a higher gradient signal early in the training. Because a successful training will end when $$D(G(z)) ≈ 0.5$$ (i.e. $$D$$ is unable to differentiate a generated sample from a real sample), the low slope for $$D(G(z)) ≈ 1$$ doesn't slow training. 

This approach is known as the *non-saturating trick* and ensures that $$G$$ and $$D$$ receive a stronger gradient when they are "weaker" than their counterpart. 

You can find a more detailed explanation of the saturating cost trick in [Goodfellow's NIPS tutorial](https://arxiv.org/pdf/1701.00160.pdf) (2016).

#### Keeping D up-to-date

According to the non-saturating graph above, $$G$$ doesn't undergo a strong gradient when it easily fools $$D$$ (i.e. in expectation $$D(G(z)) ≈ 1$$.) Thus, it is strategic to ensure $$D$$ is "stronger" when updating $$G$$. This is achievable by updating $$D$$ $$k$$ times more than $$G$$, with $$k>1$$.  

There exist many other tricks to successfully train GANs, this [repository](https://github.com/soumith/ganhacks) contains helpful supplements to our class. 

### Examples of GAN applications and nice results

GANs have been applied in myriads of applications including compressing and reconstructing images for efficient memory storage, [generating super-resolution images](https://arxiv.org/pdf/1609.04802.pdf), [generating super-resolution images](https://arxiv.org/pdf/1809.00219.pdf), [preserving privacy for clinical data sharing](https://www.biorxiv.org/content/biorxiv/early/2018/12/20/159756.full.pdf), [generating images based on text descriptions](http://proceedings.mlr.press/v48/reed16.pdf), [converting maps images corresponding satellite images, street scene translation](https://papers.nips.cc/paper/6672-unsupervised-image-to-image-translation-networks.pdf), [mass customization of medical products
such as dental crowns](https://arxiv.org/pdf/1804.00064.pdf), [cracking enciphered language data](https://arxiv.org/pdf/1801.04883.pdf) and many more. Let's delve into some of them.

#### Operation on latent codes

The latent space of random noise input, from which $$G$$ maps to the real sample space, usually contains sound meanings. For instance, [Radford et al.](https://arxiv.org/pdf/1511.06434.pdf) show that inputing in $$G$$ the latent code $$z = z_{\text{a man wearing glasses}} + z_{\text{a man without glasses}} - z_{\text{a woman without glasses}}$$ leads to a generated image $$G(z)$$ representing "a woman wearing glasses."

{% include image.html description="" link="" image="lecture/4/latent_code.png" caption="false"%}

### Generating super-resolution (SR) images

There is promising research in GANs to recover high-resolution (HR) image from low-resolution (LR) images. Specifically, it is challenging to recover the finer texture details when super-resolving at large upscaling factors. 

Practical applications of SR includes fast-moving vehicles identification, [number plates reading](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.2810&rep=rep1&type=pdf), biomedical applications such as accurate measurement and visualization of structure in living tissues, and biometrics regnition, to name a few.

The following picture compares different SR algorithms' outcome. The ground thruth is the left-most picture.

{% include image.html description="" link="" image="lecture/4/super-resolution.png" caption="false"%}

Here's a picture generated by [a project award winner](http://cs230.stanford.edu/projects_fall_2018/reports/12365342.pdf). From left to right: 32x32 LR input, SRPGGAN Output (256x256) and HR ground-truth (256x256.)

{% include image.html description="" link="" image="lecture/4/res_GAN.png" caption="false"%}

### Image to Image translation via Cycle-GANs

Translating images between different domains has been an important application of GANs. For example, [CycleGANs](https://arxiv.org/pdf/1703.10593.pdf) translate horses into zebras, apples to oranges, summer features into winter features and vice-versa.

{% include image.html description="" link="" image="lecture/4/cyclegan.png" caption="false"%}

They consist of two pairs of generator-discriminator players: $$(G_1,D_1)$$ and $$(G_2,D_2)$$. 

The goal of the $$(G_1, D_1)$$ game is to turn domain 1 samples into domain 2 samples. In contrast, the goal of $$(G_2, D_2)$$ game is to turn domain 2 samples into domain 1 samples. 

A necessary cycle constraint imposes that the composition of $$G_1$$ and $$G_2$$ results in the identity function, to ensure that the non-changing features (non-horse or zebra elements) are saved during the translation.

The training of this four-player game is summarized in five cost functions:

$$D_1$$'s cost: 

$$J^{(D_1)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} \log (D_1(z^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D_1(G_1(H^{(i)})))$$

$$G_1$$'s cost: 

$$ J^{(G_1)} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D_1(G_1(H^{(i)})))$$

$$D_2$$'s cost: 

$$J^{(D_2)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} \log (D_2(h^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D_2(G_2(Z^{(i)})))$$

$$G_2$$'s cost: 

$$ J^{(G_2)} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D_2(G_2(Z^{(i)})))$$

Cycle-consistent cost: 

$$ J^{\text{cycle}} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \Vert G_2(G_1(H^{(i)})) - H^{(i)} \Vert_1 - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \Vert G_1(G_2(Z^{(i)})) - Z^{(i)} \Vert_1 $$

### References

[1] Tero Karras, Samuli Laine, Timo Aila: A Style-Based Generator Architecture for Generative Adversarial Networks (2019)

[2] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio: Generative Adversarial Networks (2014)

[3] Diederik P. Kingma, Jimmy Ba: Adam: A Method for Stochastic Optimization (2014)

[4] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi: Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (2016)

[5] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang: ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (2018)

[6] Brett K. Beaulieu-Jones, Zhiwei Steven Wu, Chris Williams, Ran Lee, Sanjeev P. Bhavnani, James Brian Byrd, Casey S. Greene: Privacy-preserving generative deep neural networks support clinical data sharing (2017)

[7] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee: Generative Adversarial Text to Image Synthesis (2016)

[8] Ming-Yu Liu, Thomas Breuel, Jan Kautz: Unsupervised Image-to-Image Translation Networks (2017)

[9] Jyh-Jing Hwang, Sergei Azernikov, Alexei A. Efros, Stella X. Yu: Learning Beyond Human Expertise with Generative Models for Dental Restorations (2018)

[10] Aidan N. Gomez, Sicong Huang, Ivan Zhang, Bryan M. Li, Muhammad Osama, Lukasz Kaiser: Unsupervised Cipher Cracking Using Discrete GANs (2018)

[11] Alec Radford, Luke Metz, Soumith Chintala: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2015)

[12] Yuan Jie, Du Si-dan, Zhu Xiang: Fast Super-resolution for License Plate Image Reconstruction  (2008)

[13] Yujie Shu: Human Portrait Super Resolution Using GANs (2018)

[14] Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (2017)
