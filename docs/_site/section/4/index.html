<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Section 4 (Week 4)</title>

	<meta name="description" content="Xavier Initialization and Regularization">


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/papers">Papers</a></li>
			
				<li><a href="/projects">Projects</a></li>
			
				<li><a href="/sections">Sections</a></li>
			
				<li><a href="/lectures">Lectures</a></li>
			
				<li><a href="/blogs">Blogs</a></li>
			
				<li><a href="/notes">Notes</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							Deep Learning
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/papers">Papers</a></li>
							
								<li><a href="/projects">Projects</a></li>
							
								<li><a href="/sections">Sections</a></li>
							
								<li><a href="/lectures">Lectures</a></li>
							
								<li><a href="/blogs">Blogs</a></li>
							
								<li><a href="/notes">Notes</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Section 4 (Week 4)</h1>
								
								
									<p class="hero-subheader__desc">Xavier Initialization and Regularization</p>
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<p>Training a deep neural network involves finding the right values for the weights: we would like to end with a model with good <strong>generalization error</strong>. Our model should perform well on the full, real-world data distribution, and should neither underfit nor overfit.</p>

<p>In this section, we’ll analyze two methods, initialization and regularization, and show how they help us train models more effectively.</p>

<h1 id="xavier-initialization">Xavier Initialization</h1>

<p>Last week, we discussed backpropagation and gradient descent for deep learning models. All deep learning optimization methods involve an initialization of the weight parameters.</p>

<p>Let’s explore the <strong><a href="https://www.deeplearning.ai/ai-notes/initialization/index.html" target="_blank">first visualization in this article</a></strong> to gain some intuition on the effect of different initializations.</p>

<p><strong>What makes a good or bad initialization? How can different magnitudes of initializations lead to exploding and vanishing gradients?</strong></p>

<p><strong>If we initialize weights to all zeros or the same value, what problem arises?</strong></p>

<table class="image">
<caption align="bottom"><small>Visualizing the effects of different initializations. (<a href="https://www.deeplearning.ai/ai-notes/initialization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/4/viz.png" style="" /></td></tr>
</table>

<p>The goal of Xavier Initialization is to initialize the weights such that the variance of the activations are the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing.</p>

<p>To help derive our initialization values, we will make the following <strong>simplifying assumptions</strong>:</p>

<ul>
  <li>Weights and inputs are centered at zero</li>
  <li>Weights and inputs are independent and identically distributed</li>
  <li>Biases are initialized as zeros</li>
  <li>We use the tanh() activation function, which is approximately linear with small inputs: <script type="math/tex">Var(a^{[l]}) \approx Var(z^{[l]})</script></li>
</ul>

<p><strong>Let’s derive Xavier Initialization now, step by step.</strong></p>

<p>Our full derivation gives us the following initialization rule, which we apply to all weights:</p>

<script type="math/tex; mode=display">W^{[l]}_{i,j} = \mathcal{N}\left(0,\frac{1}{n^{[l-1]}}\right)</script>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/section/5/proof.png" style="" /></td></tr>
</table>

<p>Xavier initialization is designed to work well with tanh or sigmoid activation functions. For ReLU activations, look into He initialization, which follows a very similar derivation.</p>

<h1 id="l1-and-l2-regularization">L1 and L2 Regularization</h1>

<p>We know that <script type="math/tex">L_1</script> regularization encourages sparse weights (many zero values), and that <script type="math/tex">L_2</script> regularization encourages small weight values, but <strong>why does this happen?</strong></p>

<p>Let’s consider some cost function <script type="math/tex">J(w_1,\dots,w_l)</script>, a function of weight matrices <script type="math/tex">w_1,\dots,w_l</script>. Let’s define the following two regularized cost functions:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
J_{L_1}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^l|w_i|\\
J_{L_2}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^l||w_i||^2
\end{align} %]]></script>

<p><strong>Let’s derive the gradient updates for these cost functions.</strong></p>

<p>The update for <script type="math/tex">w_i</script> when using <script type="math/tex">J_{L_1}</script> is:</p>

<script type="math/tex; mode=display">w_i^{k+1} = w_i^{k} - \underbrace{\alpha\lambda sign(w_i)}_{L_1 \text{ penalty}} - \alpha\frac{\partial J}{\partial w_i}</script>

<p>The update for <script type="math/tex">w_i</script> when using <script type="math/tex">J_{L_2}</script> is:</p>

<script type="math/tex; mode=display">w_i^{k+1} = w_i^{k} - \underbrace{2\alpha\lambda w_i}_{L_2 \text{ penalty}}- \alpha\frac{\partial J}{\partial w_i}</script>

<p><strong>What do you notice that is different between these two update rules, and how does it affect optimization? What effect does the hyperparameter <script type="math/tex">\lambda</script> have?</strong></p>

<table class="image">
<caption align="bottom"><small>A histogram of weight values for an unregularized (red) and L1 regularized (blue left) and L2 regularized (blue right) network. (<a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/6/histogram.jpeg" style="" /></td></tr>
</table>

<p>The different effects of <script type="math/tex">L_1</script> and <script type="math/tex">L_2</script> regularization on the optimal parameters are an artifact of the different ways in which they change the original loss landscape. In the case of two parameters (<script type="math/tex">w_1</script> and <script type="math/tex">w_2</script>), we can <a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" target="_blank"><strong>visualize this</strong></a>.</p>

<table class="image">
<caption align="bottom"><small>The landscape of a two parameter loss function with L1 regularization (left) and L2 regularization (right). (<a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/6/loss.jpeg" style="" /></td></tr>
</table>

<p>For a deeper dive into regularization, take a look at this longer <a href="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" target="_blank">blog post</a>, as well as Chapter 3 of <a href="https://web.stanford.edu/~hastie/ElemStatLearn/" target="_blank">The Elements of Statistical Learning</a>.</p>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2020. - Pedro Abundio Wang <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
