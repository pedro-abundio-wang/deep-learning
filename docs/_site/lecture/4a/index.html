<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Lecture 4</title>


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/papers">Papers</a></li>
			
				<li><a href="/projects">Projects</a></li>
			
				<li><a href="/sections">Sections</a></li>
			
				<li><a href="/lectures">Lectures</a></li>
			
				<li><a href="/blogs">Blogs</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							Deep Learning
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/papers">Papers</a></li>
							
								<li><a href="/projects">Projects</a></li>
							
								<li><a href="/sections">Sections</a></li>
							
								<li><a href="/lectures">Lectures</a></li>
							
								<li><a href="/blogs">Blogs</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Lecture 4</h1>
								
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<h1 id="lecture-4-vulnerabilities-of-neural-networks">Lecture 4: Vulnerabilities of Neural Networks</h1>
<h4 id="kian-katanforoosh">Kian Katanforoosh</h4>

<p>Neural networks are vulnerable. Online platforms run powerful neural networks-based classifiers to flag violent or sexual content for removal. A malicious attacker can circumvent these classifiers by introducing imperceptible perturbations on these images, allowing them to live on the platform! This is called an Adversarial Attack.</p>

<h2 id="attacking-neural-networks-with-adversarial-examples">Attacking Neural Networks with Adversarial Examples</h2>
<h3 id="motivation">Motivation</h3>
<p>In 2013, Szegedy et. al [1]](https://arxiv.org/pdf/1312.6199.pdf) identified that it is possible to create a fake datapoint (image <a href="https://arxiv.org/pdf/1312.6199.pdf">2</a>, text <a href="https://arxiv.org/pdf/1707.02812.pdf">3</a> <a href="http://www.aclweb.org/anthology/P18-2006">4</a>, speech <a href="https://arxiv.org/pdf/1801.01944.pdf">5</a> or even structured data <a href="http://patrickmcdaniel.org/pubs/esorics17.pdf">6</a>) to fool a classifier. In other words, you can generate a fake image that the neural network classifies as a target class you have chosen, for example a cat.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/lecture/4a/fake_image.png" style="" /></td></tr>
</table>

<p>The generated images might even look real to humans. For instance, an impercetible perturbation on a cat image fools a model to classify it as a cat, while the image still looks like a cat to human.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/lecture/4a/cat_to_iguana.png" style="" /></td></tr>
</table>

<p>These observations have deep consequences on productization neural networks. Here are some examples to be aware of:</p>

<ul>
  <li>Autonomous cars use object detectors to localize multiple objects including pedestrians, cars and traffic signs. An imperceptible perturbation of the ‘STOP’ sign image processed by the object detector could  a ‘70 miles’ speed limit. This could lead to serious trouble in a real-life setting.</li>
  <li>Face identification systems screen individuals to allow or refuse entrance to a building. A forged image perturbation of an unauthorized person’s face could make this person be authorized.</li>
  <li>Websites run powerful classifiers to detect sexual and violent content online. Imperceptible perturbations of prohibited images could bypass these classifiers and lead to false negative predictions.</li>
</ul>

<p>We will first present adversarial attacks, then delve into defenses.</p>

<h3 id="adversarial-attacks">Adversarial Attacks</h3>
<h4 id="general-procedure">General procedure</h4>
<p>Consider a model pre-trained on ImageNet <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">7</a>. The following framework is a quick way to forge an example which will be classified as <script type="math/tex">i</script>.</p>

<ol>
  <li>Pick a sample <script type="math/tex">x</script> and forward propagate it in the model to get prediction <script type="math/tex">\hat{y}</script>.</li>
  <li>Define a loss function that quantifies the distance between <script type="math/tex">\hat{y}</script> and the target class prediction <script type="math/tex">y_j</script>. For example:
<script type="math/tex">\mathcal{L}(\hat{y}, y_j) = \frac{1}{2}||\hat{y} - y_i||^2</script></li>
  <li>While keeping the parameters of the neural network frozen, minimize the defined loss function by updating the pixels of <script type="math/tex">x</script> iteratively via gradient descent (<script type="math/tex">x = x - \alpha \frac{\partial \mathcal{L}}{\partial x}</script>). You will find <script type="math/tex">x_{adv}</script> that is classified as class <script type="math/tex">y_i</script> with high probability.</li>
</ol>

<p>Because the only constrain in the loss function involved the output prediction, the forged image might be any image from the input space. In fact, the solution of the optimization problem above is much more likely to look random than real. Recall that the size of the space of 32x32 colored images is <script type="math/tex">255^(32 \times 32 \times 3) ≈ 10^{7400}</script> which is much larger than the space of 32x32 colored real images.</p>

<p>However, you can alleviate that issue by adding a term to the loss that forces the image <script type="math/tex">x</script> to be close to a chosen image <script type="math/tex">x_j</script>. This is an example of a relevant loss function:</p>

<script type="math/tex; mode=display">\mathcal{L}(\hat{y}, y_j, x) = \frac{1}{2}||\hat{y} - y_i||^2 + \lambda \cdot ||x - x_j||^2</script>

<p>where you can tune hyperparameter <script type="math/tex">\lambda</script> to balance the trade-off between the two terms of the loss expression.</p>

<p>After running the optimization process with this procedure, you find an adversarial example that looks like <script type="math/tex">x_j</script> but is classified as <script type="math/tex">y_i</script>.</p>

<h4 id="in-practice">In practice</h4>
<p>Using additional tricks such as gradient clipping, you can ensure that the differencec between the generated image and the target image (<script type="math/tex">x_j</script>) are imperceptible.</p>

<h3 id="defenses-to-adversarial-attacks">Defenses to adversarial attacks</h3>
<h4 id="types-of-attack">Types of attack</h4>
<p>There are two types of attacks:</p>
<ul>
  <li>white-box: The attacker has access to information about the network.</li>
  <li>black-box: The attacker does not have access to the network, but can query it (i.e. send inputs and observe predictions.)</li>
</ul>

<p>In the optimization above, computing <script type="math/tex">\frac{\partial \mathcal{L}}{\partial x}</script> using backpropagation requires access to the network weights. However, in a black-box setting, you could approximate the gradient by making tiny changes to the input and observing the output (<script type="math/tex">\frac{\partial \mathcal{L}}{\partial x} ≈ \frac{f(x+\varepsilon) - f(x)}{(x + \varepsilon) - x}</script>)</p>

<h4 id="defense-methods">Defense methods</h4>
<p>Defenses against adversarial attacks is an important research topic. Although no method have been proven to counter all attacks yet, the following directions have been proposed:</p>
<ol>
  <li>Create a SafetyNet <a href="https://arxiv.org/pdf/1704.00103.pdf">8</a> acting as a FireWall to stop adversarial examples from fooling your model. SafetyNet should be hard to optimized such that it is difficult to produce examples that are both misclassified and slip past SafetyNet’s detector.</li>
  <li>Add correctly labelled adversarial examples to the training set. This consists of simply training the initial network with additional examples to force the representation of each class to take into account small perturbations.</li>
  <li>Use Adversarial training. For each example fed to your network, the loss also takes into account the prediction of a perturbation <script type="math/tex">x_{adv}</script> of the input <script type="math/tex">x</script> of label <script type="math/tex">y</script>. In other words, by calling <script type="math/tex">\mathcal{L}</script> the ‘normal’ loss of the network, <script type="math/tex">W</script> its parameters, a possible candidate for the new loss can be given by <script type="math/tex">L(W,x,y) + \lambda L(W,x_{adv}, y)</script>. Here, <script type="math/tex">\lambda</script> is a hyperparameter that balances model robustness versus performance towards real data.</li>
</ol>

<p>Note that (2) and (3) are computationally expensive using the iterative method of generating adversarial example. In the next part, you will learn a method called Fast Gradient Sign Method (FGSM) which generates adversarial examples in one pass.</p>

<h3 id="why-are-neural-networks-vulnerable-to-adversarial-examples">Why are neural networks vulnerable to adversarial examples?</h3>
<h4 id="example-adversarial-attack-on-logistic-regression">Example: Adversarial attack on logistic regression</h4>
<p>Consider a logistic regression defined by: <script type="math/tex">\hat{y} = \sigma{Wx + b}</script>, where <script type="math/tex">x \mapsto \sigma(x)</script> indicates the sigmoid function. The shapes are: <script type="math/tex">x \in \mathbb{R}^{n \times 1}</script>, <script type="math/tex">W \in \mathbb{R}^{1 \times n}</script> and <script type="math/tex">\hat{y} \in \mathbb{R}^{1 \times 1}</script>. For simplicity, assume <script type="math/tex">b = 0</script> and <script type="math/tex">n = 6</script>.</p>

<p>Let <script type="math/tex">W = (1, 3, -1, 2, 2, 3)</script> and <script type="math/tex">x = (1, -1, 2, 0, 3, -2)^T</script>. In this setting, <script type="math/tex">\hat{y} = 0.27</script>. This means that with <script type="math/tex">73\%</script> probability, the predicted class is <script type="math/tex">y = 0</script>.</p>

<p>Now, how can you change <script type="math/tex">x</script> slightly and convert the model’s prediction to <script type="math/tex">y = 1</script>? One way is to move components of <script type="math/tex">x</script> in the same direction as <script type="math/tex">sign(W)</script>. By doing this, you ensure that the perturbations will lead to a positive addition to <script type="math/tex">\hat{y}</script>.</p>

<p><script type="math/tex">x_{adv} = x + \varepsilon sign(W) = (1, -1, 2, 0, 3, -2)^T + (0.4, 0.4, -0.4, 0.4, 0.4, 0.4) = (1.4, -0.6, 1.6, 0.4, 3.4, -1.6)</script> for <script type="math/tex">\epsilon = 0.4</script> leads to <script type="math/tex">\hat{y} = \sigma{Wx_{adv} + b} = \sigma (1, 3, -1, 2, 2, 3) \cdot (1.4, -0.6, 1.6, 0.4, 3.4, -1.6)  = \sigma(1.4 - 1.8 - 1.6 + 0.8 + 6.8 - 4.8) = \sigma(0.8) = 0.69</script>. The prediction is now <script type="math/tex">y = 1</script> with 69% confidence.</p>

<p>This small example illustrates that a rightly chosen <script type="math/tex">\epsilon</script>-perturbation  can have a considerable impact on the output of a neural network.</p>

<h4 id="comments">Comments</h4>
<ul>
  <li>Despite being less powerful, using <script type="math/tex">\varepsilon sign(W)</script> instead of <script type="math/tex">\varepsilon W</script> ensures that <script type="math/tex">x_{adv}</script> stays close to <script type="math/tex">x</script>. The perturbation is indeed capped by <script type="math/tex">\varepsilon</script>.</li>
  <li>The larger <script type="math/tex">n</script>, the more powerful the attack. Indeed, perturbating each feature of <script type="math/tex">x</script> additively impacts <script type="math/tex">\hat{y}</script>.</li>
  <li>For neural networks, <script type="math/tex">x_{adv} = x + \varepsilon sign(W)</script> can be generalized to <script type="math/tex">x_{adv} = x + \varepsilon sign(\nabla_x \mathcal{L}(W,x,y))</script>. The intuition is that you want to push <script type="math/tex">x</script> with limited amplitude in the direction of positive changes of the loss function. This method is called the <strong>Fast Gradient Sign Method (FGSM)</strong>. It generates adversarial examples much faster than the iterative method described earlier.</li>
</ul>

<h4 id="conclusion">Conclusion</h4>

<p>Although early attempts at explaining the existence of adversarial examples focused on overfitting and network non-linearity, Goodfellow et al. (in Explaining and Harnessing Adversarial Examples [cite]) found that it is due to the linearity of the network. Indeed, neural networks have been designed to behave linearly (think about the introduction of ReLU, LSTMs, etc.) in order to ease to optimization process. Even activation functions such as <script type="math/tex">\sigma</script> and <script type="math/tex">tanh</script> have been designed to function in their linear regime with methods such as Xavier Initialization and BatchNorm. However, the FGSM illustrates that easy-to-train models are easy-to-attack because the gradients can backpropagate.</p>

<h4 id="further-readings">Further readings</h4>
<ul>
  <li>If you’re looking for a survey of adversarial examples, Yuan et al. <a href="https://arxiv.org/pdf/1712.07107.pdf">9</a> offer a review of recent findings on adversarial examples for deep neural networks.</li>
  <li>If you are more interested in adversarial attack and defense scenarios, Kurakin et al. <a href="https://arxiv.org/pdf/1804.00097.pdf">10</a> is a great supplement.</li>
</ul>

<p>References:
[1] 
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]</p>

<p>(scribe: …)</p>

						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2019. - Pedro Abundio Wang <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
