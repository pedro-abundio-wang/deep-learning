<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Section 6 (Week 7)</title>

	<meta name="description" content="Advanced Evaluation Metrics">


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/papers">Papers</a></li>
			
				<li><a href="/projects">Projects</a></li>
			
				<li><a href="/sections">Sections</a></li>
			
				<li><a href="/lectures">Lectures</a></li>
			
				<li><a href="/blogs">Blogs</a></li>
			
				<li><a href="/notes">Notes</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							Deep Learning
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/papers">Papers</a></li>
							
								<li><a href="/projects">Projects</a></li>
							
								<li><a href="/sections">Sections</a></li>
							
								<li><a href="/lectures">Lectures</a></li>
							
								<li><a href="/blogs">Blogs</a></li>
							
								<li><a href="/notes">Notes</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Section 6 (Week 7)</h1>
								
								
									<p class="hero-subheader__desc">Advanced Evaluation Metrics</p>
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<h1 id="introduction">Introduction</h1>

<p>Deep learning tasks can be complex and hard to measure: how do we know whether one network is better than another? In some simpler cases such as regression, the loss function used to train a network can be a good measurement of the network’s performance. However, for many real-world tasks, there are <strong>evaluation metrics</strong> that encapsulate, in a single number, how well a network is doing in terms of real world performance. These evaluation metrics allow us to quickly see the quality of a model, and easily compare different models on the same tasks.</p>

<p>Next, we’ll go through some case studies of different tasks and their metrics.</p>

<h1 id="warmup-classification-and-the-f1-score">Warmup: Classification and the F1 Score</h1>

<p>Let’s consider a simple binary classification problem, where we are trying to predict if a patient is healthy or has pneumonia. We have a test set with 10 patients, where 9 patients are healthy (shown as green squares) and 1 patient has pneumonia (shown as a red square).</p>

<table class="image">
<caption align="bottom"><small>Ground truth for your test set. (<a href="">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/9/truth.png" style="" /></td></tr>
</table>

<p>We trained 3 models for this task (Model1, Model2, Model3), and we’d like to compare the performance of these models. The predictions from each model on the test set are shown below:</p>

<table class="image">
<caption align="bottom"><small>Results of your three models on the test set. (<a href="">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/9/models.png" style="" /></td></tr>
</table>

<h2 id="accuracy">Accuracy</h2>

<p>To compare the models, we could first use accuracy, which is the number of correctly classified examples divided by the total:</p>

<script type="math/tex; mode=display">\text{accuracy}(f) = \frac{\sum_{x_i \in X_{test}} \mathbb{1}\{f(x_i) = y_i\}}{\mid X_{test} \mid}</script>

<p>If we use accuracy as your evaluation metric, it seems that the best model is Model1.</p>

<script type="math/tex; mode=display">\text{Accuracy}(M_1) = \frac{9}{10} \qquad \text{Accuracy}(M_2) = \frac{8}{10} \qquad \text{Accuracy}(M_3) = \frac{5}{10}</script>

<p>In general, when you have class imbalance (which is most of the time!), accuracy is not a good metric to use.</p>

<h2 id="confusion-matrix">Confusion Matrix</h2>

<p>Accuracy doesn’t discriminate between errors (i.e., it treats misclassifying a patient with pneumonia as healthy the same as misclassifying a visualizing patient with having pneumonia). A confusion matrix is a tabular format for showing a more detailed breakdown of a model’s correct and incorrect classifications.</p>

<table class="image">
<caption align="bottom"><small>A confusion matrix for binary classification. (<a href="">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/9/confusion.png" style="" /></td></tr>
</table>

<h2 id="precision-recall-and-the-f1-score">Precision, Recall, and the F1 Score</h2>

<p>For pneumonia detection, it is crucial that we find all the patients that have pneumonia. Predicting patients with pneumonia as healthy is not acceptable (since the patients will be left untreated). Thus, a natural question to ask when evaluating our models is:</p>

<p><em>Out of all the patients with pneumonia, how many did the model predict as having pneumonia?</em></p>

<p>This metric is recall, which is expressed as the following:</p>

<script type="math/tex; mode=display">\text{Recall} = \frac{tp}{tp + fn}</script>

<p><strong>What is the recall for each model?</strong></p>

<script type="math/tex; mode=display">\text{Recall}(M_1) = \frac{0}{1} \qquad \text{Recall}(M_2) = \frac{1}{1} \qquad \text{Recall}(M_3) = \frac{1}{1}</script>

<p>Imagine that the treatment for pneumonia is very costly and therefore you would also like to make sure only patients with pneumonia receive treatment.  A natural question to ask would be:</p>

<p><em>Out of all the patients that are predicted to have pneumonia, how many actually have pneumonia?</em></p>

<p>This metric is precision, expressed as the following:</p>

<script type="math/tex; mode=display">\text{Precision} = \frac{tp}{tp + fp}</script>

<p><strong>What is the precision for each model?</strong></p>

<script type="math/tex; mode=display">\text{precision}(M_1) = \frac{0}{0} \qquad \text{precision}(M_2) = \frac{1}{3} \qquad \text{precision}(M_3) = \frac{1}{6}</script>

<p>Precision and recall are both useful, but having multiple evaluation metrics makes it difficult to directly compare models. From Andrew Ng’s machine learning book:</p>

<p><em>“Having multiple-number evaluation metrics makes it harder to compare algorithms. Better to combine them to a single evaluation metric. Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.” - Machine Learning Yearning</em></p>

<p>F1 score is a metric that combines recall and precision by taking their harmonic mean:</p>

<script type="math/tex; mode=display">F_1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}</script>

<p><strong>What is the F1 score for each model?</strong></p>

<script type="math/tex; mode=display">F_1(M_1) = 0 \qquad F_1(M_2) = \frac{1}{2} \qquad F_1 = \frac{2}{7}</script>

<p><strong>Food for thought: If F1 score is a great one-number measurement of model performance, why don’t we use it as the loss function?</strong></p>

<h1 id="object-detection-iou-ap-and-map">Object Detection: IoU, AP, and mAP</h1>

<p>In object detection, two primary metrics are used: intersection over union (IoU) and mean average precision (mAP). Let’s walk through a small example (figure credit to J. Hui’s excellent <a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">post</a>).</p>

<h2 id="intersection-over-union-iou">Intersection over Union (IoU)</h2>

<p>Object detection involves finding objects, classifying them, and localizing them by drawing bounding boxes around them. Intersection over union is an intuitive metric that measures the goodness of fit of a bounding box:</p>

<table class="image">
<caption align="bottom"><small>Intersection over Union. (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/7/iou.png" style="" /></td></tr>
</table>

<p>The higher the IoU, the better the fit. IoU is a great metric since it works well for any size and shape of object. This per-object metric, along with precision and recall, form the basis for the full object detection metric, mean average precision (mAP).</p>

<h2 id="average-precision-ap-the-area-under-curve-auc">Average Precision (AP): the Area Under Curve (AUC)</h2>

<p>Object detectors create multiple predictions: each image can have multiple predicted objects, and there are many images to run inference on. Each predicted object has a confidence assigned with it: this is how confident the detector is in its prediction.</p>

<p>We can choose different confidence thresholds to use, to decide which predictions to accept from the detector. For instance, if we set the threshold to 0.7, then any predictions with confidence greater than 0.7 are accepted, and the low confidence predictions are discarded. Since there are so many different thresholds to choose, how do we summarize the performance of the detector?</p>

<p>The answer uses a precision-recall curve. At each confidence threshold, we can measure the precision and recall of the detector, giving us one data point. If we connect these points together, one for each threshold, we get a precision recall curve like the following:</p>

<table class="image">
<caption align="bottom"><small>Precision recall curve. The line is the curve: the blue shows the area under the curve (AUC). (<a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/7/prcurve.png" style="" /></td></tr>
</table>

<p>The better the model, the higher the precision and recall at its points: this pushes the boundary of the curve (the dark line) towards the top and right. We can summarize the performance of the model with one metric, by taking the area under the curve (shown in blue). This gives us a number between 0 and 1, where higher is better. This metric is commonly known as average precision (AP).</p>

<h2 id="mean-average-precision-map">Mean Average Precision (mAP)</h2>

<p>Object detection is a complex task: we want to accurately detect all the objects in an image, draw accurate bounding boxes around each one, and accurately predict each object’s class. We can actually encapsulate all of this into one metric: mean average precision (mAP).</p>

<p>To start, let’s compute AP for a single image and class. Imagine our network predicts 10 objects of some class in an image: each prediction is a single bounding box, predicted class, and predicted confidence (how confident the network is in its prediction).</p>

<p>We start with IoU to decide if each prediction is correct or not. For a ground truth object and nearby prediction, if</p>

<ol>
  <li>the predicted class matches the actual class, and</li>
  <li>the IoU is greater than a threshold,</li>
</ol>

<p>we say that the network got that prediction right (true positive). Otherwise, the prediction is a false positive.</p>

<p>We can now sort our predictions by their confidence, descending, resulting in the following table:</p>

<table class="image">
<caption align="bottom"><small>Table of predictions, from most confident to least confident. Cumulative recision and recall shown on the right. (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/7/table1.png" style="" /></td></tr>
</table>

<p>For each confidence level (starting from largest to smallest), we compute the precision and recall up to that point. If we graph this, we get the raw precision-recall curve for this image and class:</p>

<table class="image">
<caption align="bottom"><small>The raw precision-recall curve. (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/7/graph1.png" style="" /></td></tr>
</table>

<p>Notice how our precision-recall curve is jagged: this is due to some predictions being correct (increasing recall) and others being incorrect (decreasing precision). We smooth out the kinks in this graph to produce our network’s final PR curve for this image and class:</p>

<table class="image">
<caption align="bottom"><small>The smoothed precision-recall curve used to calculate average precision (area under the curve). (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173">source</a>)</small></caption> 
<tr><td><img src="/doks-theme/assets/images/section/7/graph2.png" style="" /></td></tr>
</table>

<p>The average precision (AP) for this image and class is the area under this smoothed curve.</p>

<p>To compute the mean average precision over the whole dataset, we average the AP for each image and class, giving us one single metric of our network’s performance on classification! This is the metric that is used for common object detection benchmarks such as <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">Pascal VOC</a> and <a href="http://cocodataset.org/#home">COCO</a>.</p>

<!--

## Frechet Inception Distance (Optional)

This metric compares the statistics of the generated samples and real samples. It models both distributions as multivariate Gaussian. Thus, these two distributions can be compactly represented by their mean $$\mu$$ and covariance matrix $$\Sigma$$ exclusively. That is:

$$X_r\sim N(\mu_x,\Sigma_x) \text{ and } X_g\sim(\mu_g,\Sigma_g)$$

These two distributions are estimated with 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively. 

Finally, the FID between the real image distribution ($$Xr$$) and the generated image distribution ($$Xg$$) is computed as:

$$FID(x,g) = ||\mu_x - \mu_g||_2^2 + Tr(\Sigma_x + \Sigma_g - 2(\Sigma_x\Sigma_g)^{\frac{1}{2}})$$

Therefore, lower FID corresponds to more similar real and generated samples as measured by the distance between their activation distributions.

-->

<h1 id="evaluating-translations-automatically-with-bleu-score">Evaluating Translations Automatically with BLEU Score</h1>

<p>At a glance, translations seem like a difficult domain to evaluate: many translations could be reasonable for a given input, and variations in both structure and vocabulary could have varying impacts on translation quality. The BLEU score is used to automatically compare generated translations to reference translations, and does a good approximation in practice.</p>

<h2 id="regular-precision-on-unigrams">Regular Precision on Unigrams</h2>

<p>Let’s say we want to translate the following French sentence into English:</p>

<p><code class="highlighter-rouge">Le chat est sur le tapis</code></p>

<p>Our baseline translator isn’t too great and generates the following:</p>

<p><code class="highlighter-rouge">The the the the the the the</code></p>

<p>We have these two reference (ground truth) translations in our dataset:</p>

<p>Reference 1: <code class="highlighter-rouge">The cat is on the mat</code></p>

<p>Reference 2: <code class="highlighter-rouge">There is a cat on the mat</code></p>

<p>If we tally up the unigrams (single words) in our translation, we have 7 of the word “the”.</p>

<p>For each unigram, we add 1 if it is present in at least one of the reference sentences, and 0 otherwise.</p>

<p>So, for our translation, we get a standard precision of = 7/7 = 1. This makes our translation look perfect, which is definitely not good!</p>

<h2 id="modified-precision-for-unigrams">Modified Precision for Unigrams</h2>

<p>We can improve on regular precision’s problem by counting the kth unigram of a particular word as correct only if a reference sentence has k or more of that word. This way, we will penalize for returning too many duplicate words.</p>

<p>In our example, “the” appears twice in reference 2, and one time on reference 1.</p>

<p>So, our translation’s modified precision = 2/7. This seems like a more reasonable metric, but this doesn’t take into account where the “the” words go in the sentence.</p>

<h2 id="modified-precision-on-bigrams">Modified Precision on Bigrams</h2>

<p>Now let’s say we improve our translator model and generate this:</p>

<p><code class="highlighter-rouge">The cat the cat on the mat</code></p>

<p>Here are our two references, repeated:</p>

<p>Reference 1: <code class="highlighter-rouge">The cat is on the mat</code></p>

<p>Reference 2: <code class="highlighter-rouge">There is a cat on the mat</code></p>

<p>Let’s run modified precision on bigrams (groups of two consecutive words).</p>

<p>Counts of bigrams in our translation:</p>

<p>{“the cat”: 2, “cat the”: 1, “cat on”:1, “on the”: 1, “the mat”: 1}</p>

<p>Maximum number of times each bigram appears in one of the references:</p>

<p>{“the cat”: 1, “cat the”: 0, “cat on”:1, “on the”: 1, “the mat”: 1}</p>

<p>We sum the number of bigrams that appeared in references, and divide by total count of bigrams in our translation. The modified bigram precision  is thus 4/6.</p>

<h2 id="the-full-bleu-score">The Full BLEU Score</h2>

<p>We can generalize the modified precision to n-grams (groups of n consecutive words):</p>

<script type="math/tex; mode=display">p_n = \frac{\Sigma_{ngrams \in y_{pred}} Count(ngram \in ref) }{ \Sigma_{ngrams \in y_{pred}} Count(ngram) }</script>

<p>To get a holistic measurement of the translation, we can sum modified precision, say from unigrams to 4-grams:</p>

<script type="math/tex; mode=display">\text{Combined BLEU} =  \text{BP} * \exp(\frac{1}{4} \Sigma_{n=1}^4 p_n )</script>

<p>Notice that the precision of a translation would be higher for shorter generated translations, because fewer words have a higher chance to appear in the references. (For instance, what if our translation was “There is a cat”?)</p>

<p>Thus, we multiply BP, a brevity penalty, to the summation. This coefficient penalizes generated sentences that are shorter than the references.</p>

<script type="math/tex; mode=display">\text{BP} = 1 \text{ if generated length} > \text{reference length}</script>

<script type="math/tex; mode=display">\text{BP} = \exp(1 - \frac{\text{reference length}}{\text{generated length}}) \text{ otherwise (less than 1)}</script>

<h1 id="generative-models-and-the-inception-score">Generative Models and the Inception Score</h1>

<p>GAN loss functions usually measure how well the generator fools the discriminator. That is, the generator weights are optimized to maximize the probability that any fake image is classified by the discriminator as belonging to the real dataset.</p>

<p>However, GAN loss functions don’t measure the quality and diversity of the images generated. Thus, in order to benchmark GANs based on these two image criterions, we would need a new metric or procedure to compare them.</p>

<p>As mentioned above, we want to define a metric that returns a high score to GANs that output diverse and good quality images.</p>

<p><strong>What quantity could you maximize to check that the generated images are of high quality?</strong></p>

<p>One data driven way of making sure that the generated images are of high quality is by observing that is easy to infer the class of a high quality image with a NN classifier due to its high resolution (content of information) and their visual correspondence to the desired class (saliency). Mathematically, this is equivalent to say that the probability distribution of the class conditioned over an input image <script type="math/tex">p(y\mid x)</script> has low entropy (highly spiked, concentrated). Hence the name of the metric: for estimating <script type="math/tex">p(y\mid x)</script> the authors used a pretrained Inception network.</p>

<p><strong>How could you penalize GAN generators that don’t output diverse images?</strong></p>

<p>Diversity means having images from all the classes we are interested in. Using again the same statistical feature (the entropy), we can formulate mathematically a new figure of merit to award diversity in the generated images. How? By forcing the distribution of classes <script type="math/tex">p(y)</script> of the generated dataset to have a high entropy.</p>

<p><strong>How can be the entropy of <script type="math/tex">p(y)</script> computed?</strong></p>

<p>As we can estimate <script type="math/tex">p(y\mid x)</script> with an Inception NN, <script type="math/tex">p(y)</script> would be its marginal distribution.  As <script type="math/tex">x = G(z)</script>, <script type="math/tex">p(y)</script> can be derived integrating over <script type="math/tex">z</script> as follows:</p>

<script type="math/tex; mode=display">\int_z p(y\mid x = G(z)) dz</script>

<p><strong>How you would combine in a single metric the previous two criterions?</strong></p>

<p>Using the KL divergence. Intuitively, the KL divergence can be thought as the “distance” in terms of similarity between two distributions.</p>

<p>The Inception score (of a Generator) is defined as follows:</p>

<script type="math/tex; mode=display">IS(G) = exp(\mathbb{E}_{x \sim p_\theta} D_{KL}(p(y\mid x) \mid\mid p(y)))</script>

<p>If the images produced by the generative model are diverse and of high quality (i.e. <script type="math/tex">p(y\mid x)</script> has low entropy and <script type="math/tex">p(y)</script> has high entropy), then we expect a large KL-divergence between the distributions <script type="math/tex">p(y)</script> and <script type="math/tex">p(y\mid x)</script>, resulting in a large IS.</p>

<p>One shortcoming for IS is that it can misrepresent the performance if it only generates one image per class. <script type="math/tex">p(y)</script> will still be uniform even though the diversity is low.</p>


						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2020. - Pedro Abundio Wang <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
