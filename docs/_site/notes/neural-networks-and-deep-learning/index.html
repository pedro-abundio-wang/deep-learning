<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Neural Networks and Deep Learning</title>


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">
<script type="text/javascript" async
  src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	</head>
	<body class="blue" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/papers">Papers</a></li>
			
				<li><a href="/projects">Projects</a></li>
			
				<li><a href="/sections">Sections</a></li>
			
				<li><a href="/lectures">Lectures</a></li>
			
				<li><a href="/blogs">Blogs</a></li>
			
				<li><a href="/notes">Notes</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
							Deep Learning
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/papers">Papers</a></li>
							
								<li><a href="/projects">Projects</a></li>
							
								<li><a href="/sections">Sections</a></li>
							
								<li><a href="/lectures">Lectures</a></li>
							
								<li><a href="/blogs">Blogs</a></li>
							
								<li><a href="/notes">Notes</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Neural Networks and Deep Learning</h1>
								
								
								
								
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					
					<div class="col-md-7">
						<div class="content">
							<h2 id="introduction-to-deep-learning">Introduction to deep learning</h2>

<h3 id="what-is-a-neural-network">What is a Neural Network</h3>

<p>Letâ€™s start with the house price prediction example. Suppose that you have a dataset with six houses and we know the price and the size of these houses. We want to fit a function to predict the price of these houses with respect to its size.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/12.png" style="" /></td></tr>
</table>

<p>We will put a straight line through these data points. Since we know that our prices cannot be negative, we end up with a horizontal line that passes through 0.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/13.png" style="" /></td></tr>
</table>

<p>The blue line is the function for predicting the price of the house as a function of its size. You can think of this function as a very simple neural network. The input to the neural network is the size of a house, denoted by <script type="math/tex">x</script>, which goes into a single neuron and then outputs the predicted price, which we denote by <script type="math/tex">y</script>.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/14.png" style="" /></td></tr>
</table>

<p>If this is a neural network with a single neuron, a much larger neural network is formed by taking many of the single neurons and stacking them together.</p>

<p>A basic Neural Network with more features is ilustrated in the following image.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/15.png" style="" /></td></tr>
</table>

<h3 id="supervised-learning-with-neural-networks">Supervised learning with neural networks</h3>

<p>In supervised learning, we have some input <script type="math/tex">x</script>, and we want to learn a function mapping to some output <script type="math/tex">y</script>. Just like in the house price prediction application our input were some features of a home and our goal was to estimate the price of a home <script type="math/tex">y</script>.</p>

<p>Here are some other fields where neural networks have been applied very effectively.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/16.png" style="" /></td></tr>
</table>

<p>We might input an image and want to output an index from one to a thousand, trying to tell if this picture might be one of a thousand different image classes. This can be used for photo tagging.</p>

<p>The recent progress in speech recognition has also been very exciting. Now you can input an audio clip to a neural network and can have it output a text transcript.</p>

<p>Machine translation has also made huge strikes thanks to deep learning where now you can have a neural network input an English sentence and directly output a Chinese sentence.</p>

<p>Different types of neural networks are useful for different applications.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/17.png" style="" /></td></tr>
</table>

<ul>
  <li>In the real estate application, we use a universally <strong>Standard Neural Network</strong> architecture.</li>
  <li>For image applications weâ€™ll often use <strong>Convolutional Neural Network (CNN)</strong>.</li>
  <li>Audio is most naturally represented as a one-dimensional time series or as a one-dimensional temporal sequence. Hence, for a sequence data, we often use <strong>Recurrent Neural Network (RNN)</strong>.</li>
  <li>Language, English and Chinese, the alphabets or the words come one at a time and language is also represented as a sequence data. <strong>Recurrent Neural Network (RNN)</strong> are often used for these applications.</li>
</ul>

<p>Machine learning is applied to both <strong>Structured Data and Unstructured Data</strong>.</p>

<p>Structured Data means basically databases of data. In house price prediction, you might have a database or the column that tells you the size and the number of bedrooms.</p>

<p>In predicting whether or not a user will click on an ad, we might have information about the user, such as the age, some information about the ad, and then labels that youâ€™re trying to predict.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/18.png" style="" /></td></tr>
</table>

<p>Structured data means, that each of the features, such as a size of the house, the number of bedrooms, or the age of a user, have a very well-defined meaning. In contrast, unstructured data refers to things like audio, raw audio, or images where you might want to recognize whatâ€™s in the image or text. Here, the features might be the pixel values in an image or the individual words in a piece of text.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/19.png" style="" /></td></tr>
</table>

<p>Neural networks, computers are now much better at interpreting unstructured data as compared to just a few years ago. This creates opportunities for many new exciting applications that use speech recognition, image recognition, natural language processing of text.</p>

<h3 id="why-is-deep-learning-taking-off">Why is deep learning taking off</h3>

<p>Many of the ideas of deep learning (neural networks) have been around for decades. Why are these ideas taking off now?</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/11.png" style="" /></td></tr>
</table>

<p>In detail, even as you accumulate more data, usually the performance of older learning algorithms, such as logistic regression, <strong>plateaus</strong>. This means its learning curve flattens out, and the algorithm stops improving even as you give it more data. It was as if the older algorithms didnâ€™t know what to do with all the data we now have.</p>

<p>If you train a small neural network (NN) on the same supervised learning task, you might get slightly better performance. Finally, if you train larger and larger neural networks, you can obtain even better performance:</p>

<p>The diagram shows NNs doing better in the regime of small datasets. This effect is less consistent than the effect of NNs doing well in the regime of huge datasets. In the small data regime, traditional algorithms may or may not do better. For example, if you only have 20 training examples, it might not matter much whether you use logistic regression or a neural network; the features-engineering will have a bigger effect than the choice of algorithm. But if you have one-million examples, I would favor the neural network.</p>

<p>Biggest drivers of recent progress have been:</p>

<ul>
  <li><strong>Data availability</strong>: People are now spending more time on digital devices (laptops, mobile devices). Their digital activities generate huge amounts of data that we can feed to our learning algorithms.</li>
  <li><strong>Computational scale</strong>: We started just a few years ago, techniques (like GPUs/Powerful CPUs/Distributed computing) to be able to train neural networks that are big enough to take advantage of the huge datasets we now have.</li>
  <li><strong>Algorithm</strong>: Creative algorithms has appeared that changed the way NN works.</li>
</ul>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/20.png" style="" /></td></tr>
</table>

<p>To conclude, often you have an idea for a neural network architecture and you want to implement it in code. Fast computation is important because the process of training a neural network is very iterative and can be time-consuming. Implementing our idea then lets us run an experiment which tells us how well our neural network does. Then, by looking at it, you go back to change the details of our neural network and then you go around this circle over and over, until we get the desired performance.</p>

<h2 id="neural-networks-basics">Neural Networks Basics</h2>

<h3 id="binary-classification">Binary classification</h3>

<p>Binary classification is the task of classifying elements of a given set into two classification.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/21.png" style="" /></td></tr>
</table>

<p>A binary classification problem:</p>

<ul>
  <li>We have an input image <script type="math/tex">x</script> and the output <script type="math/tex">y</script> is a label to recognize the image.</li>
  <li>1 means cat is on an image, 0 means that a non-cat object is on an image.</li>
</ul>

<p>In binary classification, our goal is to learn a classifier that can input an image represented by its feature vector <script type="math/tex">x</script> and predict whether the corresponding label is 1 or 0. That is, whether this is a cat image or a non-cat image.</p>

<p>The computer stores 3 separate matrices corresponding to the red, green and blue (RGB) color channels of the image. If the input image is 64 by 64 pixels, then we would have three 64 by 64 matrices corresponding to the red, green and blue pixel intensity values for our image. For a 64 by 64 image - the total dimension of this vector will be 64 * 64 * 3 = 12288.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/22.png" style="" /></td></tr>
</table>

<p>Notation that we will follow is shown in the table below:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/23.png" style="" /></td></tr>
</table>

<h3 id="logistic-regression">Logistic regression</h3>

<p><strong>Logistic regression</strong> is a supervised learning algorithm that we can use when labels are either 0 or 1 and this is the so-called <strong>Binary Classification Problem</strong>. An input feature vector <script type="math/tex">x</script> may correspond to an image that we want to recognize as either a cat picture (1) or a non-cat picture (0). That is, we want an algorithm to output the prediction which is an estimate of <script type="math/tex">y</script>:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/24.png" style="" /></td></tr>
</table>

<p>More formally, we want <script type="math/tex">\hat{y}</script> to be the chance that <script type="math/tex">\hat{y}</script> is equal to 1, given the input features <script type="math/tex">x</script>. In other words, if <script type="math/tex">x</script> is a picture, we want <script type="math/tex">y</script> to tell us what is the chance that this is a cat picture.</p>

<p>The <script type="math/tex">x</script> is an <script type="math/tex">n^x</script> - dimensional vector. The parameters of logistic regression are <script type="math/tex">w</script>, which is also an <script type="math/tex">n^x</script> - dimensional vector together with <script type="math/tex">b</script> wich is a real number.</p>

<p>Given an input <script type="math/tex">x</script> and the parameters <script type="math/tex">w</script> and <script type="math/tex">b</script>, how do we generate the output <script type="math/tex">\hat{y}</script>. One thing we could try, that doesnâ€™t work, would be to have: <script type="math/tex">\hat{y} = w^T x + b</script> which is a linear function of the input and in fact, this is what we use if we were doing <strong>Linear Regression</strong>.</p>

<p>However, this is not a very good algorithm for binary classification, because we want <script type="math/tex">\hat{y}</script> to be the chance that <script type="math/tex">y</script> is equal to 1, so <script type="math/tex">\hat{y}</script> should be between 0 and 1.</p>

<p>It is difficult to enforce this because <script type="math/tex">w^T x + b</script> can be much bigger than 1 or can even be negative which doesnâ€™t make sense for a probability that we want to be in a range between 0 and 1. We can conclude that we need a function which will transform <script type="math/tex">\hat{y} = w^T x + b</script> to be in a range between 0 and 1.</p>

<p>Letâ€™s see one function that can help us do that. In logistic regression, the output is going to be the <strong>Sigmoid Function</strong>. We can see that it goes smoothly from 0 up to 1.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/25.jpeg" style="" /></td></tr>
</table>

<ul>
  <li>use <script type="math/tex">z</script> to denote the following quantity <script type="math/tex">w^T x + b</script>.</li>
  <li>we have: <script type="math/tex">\hat{y} = \sigma(w^T x + b)</script>.</li>
  <li>if <script type="math/tex">z</script> is a large positive number: <script type="math/tex">\sigma(z) = 1</script></li>
  <li>if <script type="math/tex">z</script> is a large negative number: <script type="math/tex">\sigma(z) = 0</script></li>
</ul>

<p>When we implement logistic regression, our job is to try to learn parameters <script type="math/tex">w</script> and <script type="math/tex">b</script>, so that <script type="math/tex">\hat{y}</script> becomes a good estimate of the chance of <script type="math/tex">y</script> being equal to 1.</p>

<h3 id="logistic-regression-cost-function">Logistic regression cost function</h3>

<p>First, to train parameters ğ‘¤ and ğ‘ of a logistic regression model we need to define a <strong>cost function</strong>.</p>

<p>Given a training set of ğ‘š training examples, we want to find parameters ğ‘¤ and ğ‘, so that ğ‘¦Ì‚ is as close to ğ‘¦ (ground truth).</p>

<p>Here, we will use (ğ‘–) superscript to index different training examples.</p>

<p>Henceforth, we will use <strong>loss (error) function</strong> to measure how well our algorithm is doing. The loss function is applied only to a single training sample, and commonly used loss function is a <strong>squared error</strong>:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/26.png" style="" /></td></tr>
</table>

<p>In logistic regression squared error loss function is not an optimal choice. It results in an optimization problem which is not convex, and the gradient descent algorithm may not work well, it may not converge optimally.</p>

<p>In terms of a surface, the surface is convex if, loosely speaking, it looks like a parabola. If you have a ball and let it roll along the surface, that surface is convex if that ball is guaranteed to always end up at the same point in the end. However, if the surface has bumps, then, depending on where you drop the ball from, it might get stuck somewhere else. That surface is then non-convex.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/27.png" style="" /></td></tr>
</table>

<p>To be sure that we will get to the global optimum, we will use following loss function:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/28.png" style="" /></td></tr>
</table>

<p>It will give us a convex optimization problem and it is therefore much easier to be optimized.</p>

<p>To understand why this is a good choice, letâ€™s see these two cases:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/29.png" style="" /></td></tr>
</table>

<p>A cost function measures how well our parameters ğ‘¤ and ğ‘ are doing on the entire training set :</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/30.png" style="" /></td></tr>
</table>

<ul>
  <li>Cost function ğ½ is defined as an average of a sum of loss functions of all training examples.</li>
  <li>Cost function is a function of parameters ğ‘¤ and ğ‘.</li>
</ul>

<p>In cost function diagram, the horizontal axes represent our spatial parameters, ğ‘¤ and ğ‘. In practice, ğ‘¤ can be of a much higher dimension, but for the purposes of plotting, we will illustrate ğ‘¤ and ğ‘ as scalars.</p>

<p>The cost function ğ½(ğ‘¤,ğ‘) is then some surface above these horizontal axes ğ‘¤ and ğ‘. So, the height of the surface represents the value of ğ½(ğ‘¤,ğ‘) at a certain point. Our goal will be to minimize function ğ½, and to find parameters ğ‘¤ and ğ‘.</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>Gradient Descent is an algorithm that tries to minimize the cost function ğ½(ğ‘¤,ğ‘) and to find optimal values for ğ‘¤ and ğ‘.</p>

<p>For the purpose of illustration we will use ğ½(ğ‘¤), function that we want to minimize, as a function of one variable. To make this easier to draw, we are going to ignore ğ‘ for now, just to make this a one-dimensional plot instead of a high-dimensional plot.</p>

<p>Gradient Descent starts at an initial parameter and begins to take values in the steepest downhill direction. Function ğ½(ğ‘¤,ğ‘) is convex, so no matter where we initialize, we should get to the same point or roughly the same point.</p>

<p>After a single step, it ends up a little bit down and closer to a global otpimum because it is trying to take a step downhill in the direction of steepest descent or quickly down low as possible.</p>

<p>After a fixed number of iterations of Gradient Descent, hopefully, will converge to the global optimum or get close to the global optimum.</p>

<p>The <strong>learning rate</strong> ğ›¼ controls how big step we take on each iteration of Gradient Descent.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/31.jpeg" style="" /></td></tr>
</table>

<p>If the derivative is positive, ğ‘¤ gets updated as ğ‘¤ minus a learning rate ğ›¼ times the derivative ğ‘‘ğ‘¤.</p>

<p>We know that the derivative is positive, so we end up subtracting from ğ‘¤ and taking a step to the left. Here, Gradient Descent would make your algorithm slowly decrease the parameter if you have started off with this large value of ğ‘¤.</p>

<p>Next, when the derivative is negative (left side of the convex function),  the Gradient Descent update would subtract ğ›¼ times a negative number, and so we end up slowly increasing ğ‘¤ and we are making ğ‘¤ bigger and bigger with each successive iteration of Gradient Descent.</p>

<p>So, whether you initialize ğ‘¤ on the left or on the right, Gradient Descent would move you towards this global minimum.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/32.png" style="" /></td></tr>
</table>

<h3 id="computation-graph">Computation graph</h3>

<p>Letâ€™s say that weâ€™re trying to compute a function ğ½, which is a function of three variables ğ‘, ğ‘, and ğ‘ and letâ€™s say that function ğ½ is 3(ğ‘ + ğ‘ğ‘).</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/40.png" style="" /></td></tr>
</table>

<p>Computation of this function has actually three distinct steps:</p>

<ul>
  <li>Compute ğ‘ğ‘ and store it in the variable ğ‘¢, so ğ‘¢ = ğ‘ğ‘</li>
  <li>Compute ğ‘£ = ğ‘ + ğ‘¢,</li>
  <li>Output ğ½ is 3ğ‘£.</li>
</ul>

<p>Letâ€™s summarize:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/41.png" style="" /></td></tr>
</table>

<p>In this simple example we see that, through a left-to-right pass, you can compute the value of ğ½.</p>

<h3 id="derivatives-with-a-computation-graph">Derivatives with a Computation Graph</h3>

<p>How to figure out derivative calculations of the function ğ½.</p>

<p>Now we want using a computation graph to compute the derivative of ğ½ with respect to ğ‘£. Letâ€™s get back to our picture, but with concrete parameters.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/42.png" style="" /></td></tr>
</table>

<p>First, letâ€™s see the final change of value ğ½ if we change ğ‘£ value a little bit:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/43.png" style="" /></td></tr>
</table>

<p>We can get the same result if we know calculus:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/44.png" style="" /></td></tr>
</table>

<p>We emphasize that calculation of dğ½/dğ‘£ is one step of a back propagation. The following picture depicts <strong>forward propagation</strong> as well as <strong>backward propagation</strong>:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/45.png" style="" /></td></tr>
</table>

<p>Next, what is dğ½/dğ‘. If we increase ğ‘ from 5 to 5.001, ğ‘£ will increase to 11.001 and ğ½ will increase to 33.003. So, the increase to ğ½ is the three times the increase to ğ‘ so that means this derivative is equal to 3.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/46.png" style="" /></td></tr>
</table>

<p>One way to break this down is to say that if we change ğ‘, that would change ğ‘£ and through changing ğ‘£ that would change ğ½. By increasing ğ‘, how much ğ½ changed is also determined by dğ‘£/dğ‘. This is called a <strong>chain rule</strong> in calculus:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/49.png" style="" /></td></tr>
</table>

<p>Now, letâ€™s calculate derivative dğ½/dğ‘¢.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/47.png" style="" /></td></tr>
</table>

<p>Finally, we have to find the most important values: value of dğ½/dğ‘ and dğ½/dğ‘. Letâ€™s calculate them:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/48.png" style="" /></td></tr>
</table>

<h3 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h3>

<p>Why do we need a computation graph? To answer this question, we have to check how the computation for our neural network is organized. There are two important principles in neural network computation:</p>

<ul>
  <li>Forward pass or forward propagation step</li>
  <li>Backward pass or backpropagation step</li>
</ul>

<p>During NNâ€™s <strong>forward propagation step</strong> we compute the output of our neural network. In a binary classification case, our neural network output is defined by a variable and it can have any value from [0,1] interval.</p>

<p>In order to actually train our neural network (find parameters ğ‘¤ and ğ‘ as local optima of our cost function) we have to conduct a <strong>backpropagation step</strong>. In this way, we can compute gradients or compute derivatives. With this information, we are able to implement gradient descent algorithm for finding optimal values of ğ‘¤ and ğ‘. That way we can train our neural network and expect that it will do well on a classification task.</p>

<p>A computation graph is a systematic and easy way to represent our neural network and it is used to better understand (or compute) derivatives or neural network output.</p>

<p>The computation graph of a logistic regression looks like the following:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/04.png" style="" /></td></tr>
</table>

<p>In this example, we only have two features ğ‘¥<sub>1</sub> and ğ‘¥<sub>2</sub>. In order to compute ğ‘§, we will need to input ğ‘¤<sub>1</sub>, ğ‘¤<sub>2</sub> and ğ‘ in addition to the feature values ğ‘¥<sub>1</sub> and ğ‘¥<sub>2</sub></p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/33.png" style="" /></td></tr>
</table>

<p>After that, we can compute our ğ‘¦Ì‚ (equals sigma of ğ‘§)</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/34.png" style="" /></td></tr>
</table>

<p>Finally, we are able to compute our loss function.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/39.png" style="" /></td></tr>
</table>

<p>To reduce our loss function (remember right now we are talking only about one data sample) we have to update our ğ‘¤ and ğ‘ parameters. So, first we have to compute the loss using forward propagation step. After this, we go in the opposite direction (backward propagation step) to compute the derivatives.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/35.png" style="" /></td></tr>
</table>

<p>Having computed ğ‘‘ğ‘, we can go backwards and compute ğ‘‘ğ‘§:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/36.png" style="" /></td></tr>
</table>

<p>The final step in back propagation is to go back to compute amount of change of our parameters ğ‘¤ and ğ‘:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/37.png" style="" /></td></tr>
</table>

<p>To conclude, if we want to do gradient descent with respect to just this one training example, we would do the following updates</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/38.png" style="" /></td></tr>
</table>

<h3 id="gradient-descent-on-training-set">Gradient Descent on training set</h3>

<p>The cost function is the average of our loss function, when the algorithm outputs ğ‘<sup>(ğ‘–)</sup> for the pair (ğ‘¥<sup>(ğ‘–)</sup>,ğ‘¦<sup>(ğ‘–)</sup>).</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/50.png" style="" /></td></tr>
</table>

<p>Here ğ‘<sup>(ğ‘–)</sup> is the prediction on the ğ‘–-th training example which is sigmoid of ğ‘§<sup>(ğ‘–)</sup>, were ğ‘§<sup>(ğ‘–)</sup> = ğ‘¤<sup>ğ‘‡</sup>ğ‘¥<sup>(ğ‘–)</sup> + ğ‘</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/51.png" style="" /></td></tr>
</table>

<p>The derivative with respect to ğ‘¤<sub>1</sub> of the overall cost function, is the average of derivatives with respect to ğ‘¤<sub>1</sub> of the individual loss term,</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/53.png" style="" /></td></tr>
</table>

<p>and to calculate the derivative dğ‘¤<sub>1</sub> we compute,</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/53.png" style="" /></td></tr>
</table>

<p>This gives us the overall gradient that we can use to implement logistic regression.</p>

<p>To implement Logistic Regression, here is what we can do, if ğ‘›=2, were ğ‘› is our number of features and ğ‘š is a number of samples.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/54.png" style="" /></td></tr>
</table>

<p>After leaving the inner for loop, we need to divide ğ½, dğ‘¤<sub>1</sub>, dğ‘¤<sub>2</sub> and ğ‘ by ğ‘š, because we are computing their average.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/55.png" style="" /></td></tr>
</table>

<p>After finishing all these calculations, to implement one step of a gradient descent, we need to update our parameters ğ‘¤<sub>1</sub>, ğ‘¤<sub>2</sub>, and ğ‘.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/57.png" style="" /></td></tr>
</table>

<p>It turns out there are two weaknesses with our calculations as weâ€™ve implemented it here.</p>

<p>To implement logistic regression this way, we need to write two for loops (loop over ğ‘š training samples and ğ‘› features).</p>

<p>When implementing deep learning algorithms, having explicit for loops makes our algorithm run less efficient. Especially on larger datasets, which we must avoid. For this, we use what we call vectorization.</p>

<p>The above code should run for some iterations to minimize error. So there will be two inner loops to implement the logistic regression. Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!</p>

<h3 id="vectorization">Vectorization</h3>

<p>A vectorization is basically the art of getting rid of explicit for loops whenever possible. With the help of vectorization, operations are applied to whole arrays instead of individual elements. The rule of thumb to remember is to avoid using explicit loops in your code. Deep learning algorithms tend to shine when trained on large datasets, so itâ€™s important that your code runs quickly. Otherwise, your code might take a long time to get your result.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/58.png" style="" /></td></tr>
</table>

<h3 id="vectorizing-logistic-regression">Vectorizing Logistic Regression</h3>

<p>When we are programming Logistic Regression or Neural Networks we should avoid explicit ğ‘“ğ‘œğ‘Ÿ loops. Itâ€™s not always possible, but when we can, we should use built-in functions or find some other ways to compute it. Vectorizing the implementation of Logistic Regression  makes the code highly efficient. We will see how we can use this technique to compute gradient descent without using even a single ğ‘“ğ‘œğ‘Ÿ loop.</p>

<p>Now, we will examine the forward propagation step of logistic regression. If we have ğ‘š training examples, to make a prediction on the first example we need to compute ğ‘§ and the activation function ğ‘ as follows:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/60.png" style="" /></td></tr>
</table>

<p>To make prediction on the second training example we need to compute this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/61.png" style="" /></td></tr>
</table>

<p>The same is with prediction of third training example:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/62.png" style="" /></td></tr>
</table>

<p>So if we have ğ‘š training examples we need to do these calculations ğ‘š times. In order to carry out the forward propagation step, which means to compute these predictions for all ğ‘š training examples, there is a way to do this without needing an explicit for loop.</p>

<p>We will stack all training examples horizontally in a matrix ğ—, so that every column in matrix ğ— represents one training example:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/63.png" style="" /></td></tr>
</table>

<p>Notice that matrix ğœ” is a ğ‘›<sub>ğ‘¥</sub> Ã— 1 matrix (or a column vector), so when we transpose it we get ğœ”<sup>ğ‘‡</sup> which is a 1 Ã— ğ‘›<sub>ğ‘¥</sub> matrix (or a row vector) so multiplying  ğœ”<sup>ğ‘‡</sup> with ğ— we get a 1 Ã— ğ‘š matrix. Then we add a 1 Ã— ğ‘š matrix ğ‘ to obtain ğ™.</p>

<p>We will define matrix ğ™ by placing all ğ‘§<sup>(ğ‘–)</sup> values in a row vector:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/64.png" style="" /></td></tr>
</table>

<p>In Python, we can easily implement the calculation of a matrix ğ™:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/65.png" style="" /></td></tr>
</table>

<p>As we can see ğ‘ is defined as a scalar. When you add this vector to this real number, Python automatically takes this real number ğ‘ and expands it out to the 1 Ã— ğ‘š row vector. This operation is called <strong>broadcasting</strong>.</p>

<p>Matrix ğ€ is defined as a 1 Ã— ğ‘š, wich we also got by stacking horizontaly values ğ‘<sup>(ğ‘–)</sup> as we did with matrix ğ™:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/66.png" style="" /></td></tr>
</table>

<p>In Python, we can also calculate matrix ğ€ with one line of code as follows (if we have defined sigmoid function as above):</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/67.png" style="" /></td></tr>
</table>

<p>For the gradient computation we had to compute detivative ğ‘‘ğ‘§ for every training example:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/68.png" style="" /></td></tr>
</table>

<p>In the same way, we have defined previous variables, now we will define matrix ğğ™, where we will stack all ğ‘‘ğ‘§<sup>(ğ‘–)</sup> variables horizontally, dimension of this matrix ğğ™ is 1 Ã— ğ‘š or alternativly a ğ‘š dimensional row vector.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/69.png" style="" /></td></tr>
</table>

<p>As we know that matrices ğ€ and ğ˜ are defined as follows:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/70.png" style="" /></td></tr>
</table>

<p>We can see that ğğ™ below, all values in ğğ™ can be computed at the same time.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/71.png" style="" /></td></tr>
</table>

<p>To implement Logistic Regression on code we did this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/72.png" style="" /></td></tr>
</table>

<p>This code was non-vectorized and highly inefficent so we need to transform it. First, using vectorization, we can transform equations (âˆ—) and (âˆ—âˆ—) into one equation:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/73.png" style="" /></td></tr>
</table>

<p>The cost function is:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/74.png" style="" /></td></tr>
</table>

<p>The derivatives are:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/75.png" style="" /></td></tr>
</table>

<p>To calculate ğ‘¤ and ğ‘ we will still need following ğ‘“ğ‘œğ‘Ÿ loop.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/76.png" style="" /></td></tr>
</table>

<p>We donâ€™t need to loop through entire training set, but still we need to loop through number of iterations and thatâ€™s a ğ‘“ğ‘œğ‘Ÿ loop that we canâ€™t get rid off.</p>

<h3 id="broadcasting-in-python">Broadcasting in Python</h3>

<p>The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation. If we have a matrix ğ€ and scalar value ğ‘ then scalar ğ‘ is being stretched during the arithmetic operation into an array which is the same shape as ğ€, but that stretch is only conceptual. Numpy uses the original scalar value without making copies, so that broadcasting operations are as memory and computationally efficient as possible.</p>

<p>Adding a scalar to a row vector:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/77.png" style="" /></td></tr>
</table>

<p>Adding a scalar to a column vector:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/78.png" style="" /></td></tr>
</table>

<p>Adding a row vector to a matrix:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/79.png" style="" /></td></tr>
</table>

<p>Adding a column vector to a matrix:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/80.png" style="" /></td></tr>
</table>

<h3 id="explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</h3>

<p>One way to motivate linear regression with the mean squared error loss function is to formally assume that observations arise from noisy observations, where the noise is normally distributed as follows</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/81.png" style="" /></td></tr>
</table>

<p>Thus, we can now write out the <strong>likelihood estimators</strong> of seeing a particular ğ‘¦ for a given ğ‘¥ via</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/82.png" style="" /></td></tr>
</table>

<p>Now, according to the <strong>maximum likelihood principle</strong>, the best values of ğ‘ and ğ‘¤ are those that maximize the likelihood of the entire dataset:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/83.png" style="" /></td></tr>
</table>

<p>Estimators chosen according to the maximum likelihood principle are called <strong>Maximum Likelihood Estimators</strong>. While, maximizing the product of many exponential functions, might look difficult, we can simplify things significantly, without changing the objective, by maximizing the <strong>log</strong> of the likelihood instead.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/84.png" style="" /></td></tr>
</table>

<p>Now we just need one more assumption: that ğœ is some fixed constant. Thus we can ignore the first term because it doesnâ€™t depend on ğ‘¤ or ğ‘. Now the second term is identical to the <strong>squared error</strong> objective, but for the multiplicative constant 1/ğœ<sup>2</sup>. Fortunately, the solution does not depend on ğœ. It follows that minimizing squared error is equvalent to maximum likelihood estimation of a linear model under the assumption of additive Gaussian noise.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/85.png" style="" /></td></tr>
</table>

<h2 id="shallow-neural-networks">Shallow neural networks</h2>

<h3 id="neural-networks-overview">Neural Networks Overview</h3>

<p>Logistic Regression model</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/86.png" style="" /></td></tr>
</table>

<p>corresponds to the following computation graph:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/87.png" style="" /></td></tr>
</table>

<p>We have a feature vector ğ‘¥, parameters ğ‘¤ and ğ‘ as the inputs to the computation graph. That allows us to compute ğ‘§ which is then used to compute ğ‘ and we use ğ‘ interchangeably with the output ğ‘¦Ì‚. Finally, we can compute a loss function. A circle we draw in a Logistic Regression model, we will call a node in the Neural Networks representation. The output of every node in a Neural Network is calculated in two steps: the first compute ğ‘§ value and the second computes an ğ‘ value as we can see in the picture below:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/88.png" style="" /></td></tr>
</table>

<p>A neural network is shown in the picture below. We can see we can form a neural network is created by stacking together several node units. One stack of nodes we will call a layer.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/89.png" style="" /></td></tr>
</table>

<p>The first stack of nodes we will call Layer 1, and the second we will call Layer 2. We have two types of calculations in every node in the Layer 1, as well as in the Layer 2 ( which consists of just one node).  We will use a superscript square bracket with a number of particular layer to refer to an activation function or a node that belongs to that layer. So, a superscript [1] refers to the quantities associated with the first stack of nodes, called Layer 1. The same is with a superscript [2] which refers to the second layer. Remember also that ğ‘¥<sup>(ğ‘–)</sup> refers to an individual training example.</p>

<p>The computation graph that corresponds to this Neural Network looks like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/89.png" style="" /></td></tr>
</table>

<p>So after computing ğ‘§<sup>[1]</sup>, similarly to the logistic regression, there is a computation of ğ‘<sup>[1]</sup> and thatâ€™s sigmoid of ğ‘§[<sup>[1]</sup>. Next, we compute ğ‘§<sup>[2]</sup> using another linear equation and then compute ğ‘<sup>[2]</sup> which is the final output of the neural network. Letâ€™s remind ourselves once more that ğ‘<sup>[2]</sup> = ğ‘¦Ì‚. The key intuition to take away is that, whereas for Logistic Regression we had ğ‘§ followed by ğ‘ calculation, and in this Neural Network we just do it multiple times.</p>

<p>In the same way, in a Neural Network weâ€™ll end up doing a backward calculation that looks like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/91.png" style="" /></td></tr>
</table>

<h3 id="neural-network-representation">Neural Network Representation</h3>

<p>We will now represent a single layer Neural Network. It is a Neural network with one input layer, one hidden layer and the output layer, which is a single node layer, and it is responsible for generating the predicted value ğ‘¦Ì‚.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/92.png" style="" /></td></tr>
</table>

<p>We have the following parts of the neural network:</p>

<ul>
  <li>ğ‘¥<sub>1</sub>, ğ‘¥<sub>2</sub> and ğ‘¥<sub>3</sub> are inputs of a Neural Network. These elements are scalars and they are stacked vertically. This also represents an input layer.</li>
  <li>Variables in a hidden layer are not seen in the input set. Thus, it is called a hidden layer.</li>
  <li>The output layer consists of a single neuron only and ğ‘¦Ì‚ is the output of the neural network.</li>
</ul>

<p>In the training set we see what the inputs are and we see what the output should be. But the things in the hidden layer are not seen in the training set, so the name hidden layer just means you donâ€™t see it in the training set. An alternative notation for the values of the input features will be ğ‘<sup>[0]</sup> and the term ğ‘ also stands for activations. Refers to the values that different layers of the neural network are passing on to the subsequent layers.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/93.png" style="" /></td></tr>
</table>

<p>ğ‘<sup>[1]</sup> is a 4 Ã— 1 matrix. ğ‘<sup>[2]</sup> will be a single value scalar and this is the analogous to the output of the sigmoid function in the logistic regression.</p>

<p>When we count layers in a neural network we do not count an input layer. Therefore, this is a 2-layer neural network. The first hidden layer is associated with parameters ğ‘¤<sup>[1]</sup> and ğ‘<sup>[1]</sup>. The dimensions of these matrices are:</p>

<ul>
  <li>ğ‘¤<sup>[1]</sup> is (4,3) matrix</li>
  <li>ğ‘<sup>[1]</sup> is (4,1) matrix</li>
</ul>

<p>Parameters ğ‘¤<sup>[2]</sup> and ğ‘<sup>[2]</sup> are associeted with the second layer or actually with the output layer. The dimensions of parameters in the output layer are:</p>

<ul>
  <li>ğ‘¤<sup>[2]</sup> is (1,4) matrix</li>
  <li>ğ‘<sup>[2]</sup> is a real number</li>
</ul>

<h3 id="computing-a-neural-networks-output">Computing a Neural Networkâ€™s Output</h3>

<p>Computing an output of a Neural Network is like computing an output in Logistic Regression, but repeating it multiple times. We have said that circle in Logistic Regression, or one node in Neural Network, represents two steps of calculations. We have also said that Logistic Regression is the simplest Neural Network.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/94.png" style="" /></td></tr>
</table>

<p>We will show how to compute the output of the following neural network</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/05-a.png" style="" /></td></tr>
</table>

<p>If we look at the first node and write equations for that node, and the same we will do with the second node.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/95.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/99.png" style="" /></td></tr>
</table>

<p>Calculations for the third and fourth node look the same. Now, we will put all these equations together:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/05-b.png" style="" /></td></tr>
</table>

<p>Calculating all these equations with ğ‘“ğ‘œğ‘Ÿ loop is highly inefficient so we will  to vectorize this.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/05-c.png" style="" /></td></tr>
</table>

<p>So we can define these matrices:</p>

<p><img src="Images/96.png" alt="" /></p>

<p>To compute the output of a Neural Network we need the following four equations. For the first layer of a Neural network we need these equations:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/97.png" style="" /></td></tr>
</table>

<p>Calculating the output of the Neural Network is like calculating a Logistic Regression with parameters ğ‘Š<sup>[2]</sup> as ğ‘¤<sup>ğ‘‡</sup> and ğ‘<sup>[2]</sup> as ğ‘.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/98.png" style="" /></td></tr>
</table>

<h3 id="vectorizing-across-multiple-examples">Vectorizing across multiple examples</h3>

<p>Logistic Regression Equations</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/100.png" style="" /></td></tr>
</table>

<p>These equations tell us how, when given an input feature vector ğ‘¥, we can generate predictions.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/101.png" style="" /></td></tr>
</table>

<p>If we have ğ‘š training examples we need to repeat this proces ğ‘š times. For each training example, or for each feature vector that looks like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/102.png" style="" /></td></tr>
</table>

<p>The notation ğ‘<sup>[2] (ğ‘–)</sup> means that we are talking about activation in the second layer that comes from ğ‘–<sup>ğ‘¡â„</sup> training example. In the square parentheses we write number of a layer, and number in the  parentheses reffers to the particular training example.</p>

<p>We will now see eguations for one hidden layer neural network which is presented in the following picture.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/103.png" style="" /></td></tr>
</table>

<p>To do calculations written above, we need a for loop that would look like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/104.png" style="" /></td></tr>
</table>

<p>Now our task is to vectorize all these equations and get rid of this for loop.</p>

<p>We will recall definitions of some matrices. Martix ğ— was defined as we have put all feature vectors in columns of a matrix, actually we stacked feature vectors horizontally. Every column in matrix ğ— is a feature vector for one training example, so the dimension of this matrix is(<strong>number of features in every vector, number of training examples</strong>). Matrix ğ— is defined as follows:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/105.png" style="" /></td></tr>
</table>

<p>In the same way we can get the ğ™<sup>[1]</sup> matrix, as we stack horizontally values ğ‘§<sup>[1] (1)</sup> â€¦ ğ‘§<sup>[1] (ğ‘š)</sup>:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/106.png" style="" /></td></tr>
</table>

<p>Similiar is with  values ğ‘<sup>[1] (1)</sup> â€¦ ğ‘<sup>[1] (ğ‘š)</sup> which are the activations in the first node for paritcular training example:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/107.png" style="" /></td></tr>
</table>

<p>An element in the first row and in the first column of a matrix ğ€<sup>[1]</sup> is an activation of the first hidden unit and the first training example. In the first row of this matrix there are activations in the first hidden unit among all training examples. The same is with another rows in this matrix. Next element, element in the first row and the second column, is an activation of the first unit from second training element and so on.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/108.png" style="" /></td></tr>
</table>

<p>To conclude, in matrix ğ€<sup>[1]</sup> there are activation of the first hidden layer of a Neural Network. In every column there are activations for each training example, so number of columns in this matrix is equal to the number of training examples. In the first row of this matrix there are activations first hidden unit among all training examples.</p>

<p>Vectorized version of previous calculations looks like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/109.png" style="" /></td></tr>
</table>

<p>In the following picture we can see comparation of vectorized and non-vectorized version.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/110.png" style="" /></td></tr>
</table>

<h3 id="explanation-for-vectorized-implementation">Explanation For Vectorized Implementation</h3>

<p>Letâ€™s go through part of a forward  propagation calculation for a few examples. ğ‘¥<sup>(1)</sup>, ğ‘¥<sup>(2)</sup> and ğ‘¥<sup>(3)</sup> are input vectors, those are three examples of feature vectors or three training examples.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/111.png" style="" /></td></tr>
</table>

<p>We will ignore ğ‘<sup>[1]</sup> values, to simplify these calculations, so we have following equations:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/112.png" style="" /></td></tr>
</table>

<p>So when we multiply matrix ğ–<sup>[1]</sup> with each training example we get following calculation:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/113.png" style="" /></td></tr>
</table>

<p>So when we multiply matrix ğ–<sup>[1]</sup> with each training example we get following calculation:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/114.png" style="" /></td></tr>
</table>

<p>And if we multiply ğ–<sup>[1]</sup> with matrix ğ— we will get:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/115.png" style="" /></td></tr>
</table>

<p>If we now put back the value of ğ‘<sup>[1]</sup> in equations values are still correct. What actully happens when we add ğ‘<sup>[1]</sup> values is that we end up with Python broadcasting.</p>

<p>With these equations we have justified that ğ™<sup>[1]</sup> = ğ–<sup>[1]</sup>ğ— + ğ‘<sup>[1]</sup> is a correct vectorization.</p>

<h3 id="activation-functions">Activation functions</h3>

<p>When we build a neural network, one of the choices we have to make is what activation functions to use in the hidden layers as well as at the output unit of the Neural Network. So far, weâ€™ve just been using the sigmoid activation function but sometimes other choices can work much better. Letâ€™s take a look at some of the  options.</p>

<p><strong>sigmoid activation function</strong></p>

<p>In the forward propagation steps for Neural Network we use sigmoid function as the activation function.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/116.png" style="" /></td></tr>
</table>

<p><strong>tanh activation function</strong></p>

<p>An activation function that almost always goes better than sigmoid function is tanh function. The graphic of this function is the following one:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/117.png" style="" /></td></tr>
</table>

<p>This function is a shifted version of a ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ function but scaled between -1 and 1. If we use a tanh as the activation function it almost always works better then sigmoid function because the mean of all possible values of this function is zero. Actually, it has an effect of centering the data so that the mean of the data is close to zero rather than to 0.5 and it also makes learning easier for the next layers.</p>

<p>When solving a binary classification problem it is better to use sigmoid function because it is more natural choice because if output labels ğ‘¦ âˆˆ {0,1} then it makes sence that ğ‘¦Ì‚ âˆˆ [0,1].</p>

<p>An activation function may be different for different layers through Neural Network, but in one layer there must be one - the same activation function. We use superscripts is squar parentheses [] to denote to wich layer of a Neural Network belongs each activation function. For example, activation function ğ‘”<sup>[1]</sup> is the activation function of the first layer of the Neural Network and ğ‘”<sup>[2]</sup> is the activation function of the second layer, as presented in the following picture.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/129.png" style="" /></td></tr>
</table>

<p>When talking about ğœ(ğ‘§) and tanh(ğ‘§) activation functions, one of their downsides is that derivatives of these functions are very small for higher values of ğ‘§ and this can slow down gradient descent.</p>

<p><strong>ReLU and LeakyReLU activation function</strong></p>

<p>One other choice that is well known in Machine Learning is ReLU function. This function is commonly used activation function nowadays.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/123.png" style="" /></td></tr>
</table>

<p>There is one more function, and it is modification of ReLU function. It is a  LeakyReLU function. LeakyReLU usually works better then ReLU function. Here is a graphical representation of this function:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/124.png" style="" /></td></tr>
</table>

<h3 id="why-do-you-need-non-linear-activation-functions">Why do you need non-linear activation functions?</h3>

<p>For this shallow Neural Network:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/05-a.png" style="" /></td></tr>
</table>

<p>we have following propagation steps:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/130.png" style="" /></td></tr>
</table>

<p>If we want our activation functions to be linear functions, so that we have ğ‘”<sup>[1]</sup> = ğ‘§<sup>[1]</sup> and ğ‘”<sup>[2]</sup> = ğ‘§<sup>[2]</sup>, then these equations above become:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/128.png" style="" /></td></tr>
</table>

<p>Now, itâ€™s clear that if we use a linear activation function (identity activation function), then the Neural Network will output linear output of the input. This loses much of the representational power of the neural network as often times the output that we are trying to predict has a non-linear relationship with the inputs. It can be shown that if we use a linear activation function for a hidden layer and sigmoid function for an output layer, our model becomes logistic regression model. Due to the fact that a composition of two linear functions is linear function, our area of implementing such Neural Network reduces rapidly. Rare implementation example can be solving regression problem in machine learning (where we use linear activation function in hidden layer). Recommended usage of linear activation function is to be implemented in output layer in case of regression.</p>

<h3 id="derivatives-of-activation-functions">Derivatives of activation functions</h3>

<p><strong>Derivative of sigmoid function</strong></p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/119.png" style="" /></td></tr>
</table>

<p>We denote an activation function with ğ‘, so we have:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/120.png" style="" /></td></tr>
</table>

<p><strong>Derivative of a tahn function</strong></p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/121.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/122.png" style="" /></td></tr>
</table>

<p><strong>Derivatives of ReLU and LeakyReLU activation functions</strong></p>

<p>A derivative of a ReLU function is:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/125.png" style="" /></td></tr>
</table>

<p>The derivative of a ReLU function is undefined at 0, but we can say that derivative of this function at zero is either 0 or 1. Both solution would work when they are implemented in software. The same solution works for LeakyReLU function.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/126.png" style="" /></td></tr>
</table>

<p>Derivative of LeakyReLU function is :</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/127.png" style="" /></td></tr>
</table>

<h3 id="gradient-descent-for-neural-networks">Gradient descent for Neural Networks</h3>

<p>we will see how to implement gradient descent for one hidden layer Neural Network as presented in the picture below.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/131.png" style="" /></td></tr>
</table>

<p>Parameters for one hidden layer Neural Network are ğ–<sup>[1]</sup>, ğ‘<sup>[1]</sup>, ğ–<sup>[2]</sup> and ğ‘<sup>[2]</sup>. Number of unitis in each layer are:</p>

<ul>
  <li>input of a Neural Network is feature vector ,so the length of â€œzeroâ€ layer ğ‘<sup>[0]</sup> is the size of an input feature vector ğ‘›<sub>ğ‘¥</sub> = ğ‘›<sup>[0]</sup></li>
  <li>number of hidden units in a hidden layer is ğ‘›<sup>[1]</sup></li>
  <li>number of units in output layer is ğ‘›<sup>[2]</sup>, so far we had one unit in an output layer so ğ‘›<sup>[2]</sup></li>
</ul>

<p>As we have defined a number of units in hidden layers we can now tell what are dimension of the following matrices:</p>

<ul>
  <li>ğ–<sup>[1]</sup> is (ğ‘›<sup>[1]</sup>,ğ‘›<sup>[0]</sup>) matrix</li>
  <li>ğ‘<sup>[1]</sup> is (ğ‘›<sup>[1]</sup>,1) matrix or a column vector</li>
  <li>ğ–<sup>[2]</sup> is (ğ‘›<sup>[2]</sup>,ğ‘›<sup>[1]</sup>) matrix</li>
  <li>ğ‘<sup>[2]</sup> is (ğ‘›<sup>[2]</sup>,1) , so far ğ‘<sup>[2]</sup> is a scalar</li>
</ul>

<p>Notation:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/141.png" style="" /></td></tr>
</table>

<p>Equations for one example ğ‘¥<sup>(ğ‘–)</sup>:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/132.png" style="" /></td></tr>
</table>

<p>Assuming that we are doing a binary classification, and assuming that we have ğ‘š training examples, the cost function ğ½ is:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/133.png" style="" /></td></tr>
</table>

<p>To train parameters of our algorithm we need to perform gradient descent. When training neural network, it is important to initialize the parameters randomly rather then to all zeros. So after initializing the paramethers we get into gradient descent which looks like this:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/134.png" style="" /></td></tr>
</table>

<p>So we need equations to calculate these derivatives.</p>

<p>Forward propagation equations (remember that if we are doing a binary classification then the activation function in the output layer is a sigmoid function):</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/135.png" style="" /></td></tr>
</table>

<p>Now we will show equations in the backpropagation step:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/136.png" style="" /></td></tr>
</table>

<p>Sign âˆ— stands for element  wise multiplication.</p>

<h3 id="backpropagation-intuition">Backpropagation Intuition</h3>

<p>We will now the relation between a computation graph and these equations.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/142.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/137.png" style="" /></td></tr>
</table>

<p>We have defined a loss function the actual loss when the ground truth label is ğ‘¦, and our output is ğ‘:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/138.png" style="" /></td></tr>
</table>

<p>And corresponding derivatives are:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/139.png" style="" /></td></tr>
</table>

<p>Backprpagation grapf is a graph that describes which calculations do we need to make when we want to calculate various derivatives and do the parameters update. In the following graph we can see that it is similar to the Logistic Regression grapf except that we do those calculations twice.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/140.png" style="" /></td></tr>
</table>

<p>Firstly, we calculate ğ‘‘ğ‘<sup>[2]</sup>, ğ‘‘ğ‘§<sup>[2]</sup> and these calculations allows us to calculate ğğ–<sup>[2]</sup> and ğ‘‘ğ‘<sup>[2]</sup>. Then, as we go deeper in the backpropagation step, we calculate ğ‘‘ğ‘<sup>[1]</sup>, ğ‘‘ğ‘§<sup>[1]</sup> which allows us to calculate ğğ–<sup>[1]</sup> and ğ‘‘ğ‘<sup>[1]</sup>.</p>

<h3 id="random-initialization">Random Initialization</h3>

<p>If we have for example this shallow Neural Network:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/145.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/149.png" style="" /></td></tr>
</table>

<p>Even if we have a lot of hidden units in the hidden layer they all are symetric if we initialize corresponding parameters to zeros. To solve this problem we need to initialize randomly rather then with zeros. We can do it in the following way (we consider the same shallow neural network with 2 hidden units in the hidden layer as above):</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/146.png" style="" /></td></tr>
</table>

<p>And then we can initialize ğ‘<sub>1</sub> with zeros, because initialization of ğ‘Š<sub>1</sub> breaks the symmetry, and unit1 and unit2 will not output the same value even if we initialize ğ‘<sub>1</sub> to zero. So we have:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/147.png" style="" /></td></tr>
</table>

<p>For the output layer we have:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/148.png" style="" /></td></tr>
</table>

<p>Why do we multipy with 0.01 rather then multiplying with 100 for example? What happens if we initialize parameters randomly but with big random values?</p>

<p>If we are doing a binary classification and the activation in the output layer is sigmoid function or if use tanh activation function in the hidden layers then for a not so high input value these functions get saturated, for a not so big inputs they become constant (they output 0 or 1 for sigmoid or -1 or 1 for tanh function).</p>

<p>So, we do the initialization of parameters ğ–<sup>[1]</sup> and ğ–<sup>[2]</sup> with small random values, hence we multipy with 0.01.</p>

<p>Random initialization is used to break symmetry and make sure different hidden units can learn different things.</p>

<p>We can conclude that we must initialize our parameters with small random values.</p>

<p>Well chosen initialization values of parameters leads to:</p>

<ul>
  <li>Speed up convergence of gradient descent.</li>
  <li>Increase the likelihood of gradient descent to find lower training error rates</li>
</ul>

<h2 id="deep-neural-networks">Deep Neural Networks</h2>

<h3 id="deep-l-layer-neural-network">Deep L-layer neural network</h3>

<p>Letâ€™s make a Neural Network overview. We will see what is the simplest representation of a Neural Network and how deep representation of a Neural Network looks like.</p>

<p>We have defined a Logistic Regression as a single unit that uses sigmoid activation function. Both of these simple Neural Networks we also call shallow neural networks and they are only reasonable to be applied when classifying linearly separable classes.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/150.png" style="" /></td></tr>
</table>

<p>Slightly more complex neural network is a two layer neural network (it is a neural network with one hidden layer). This shallow neural network can classify two datasets that are not linaearly separable, but it is not good at classifying more compelex datasets.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/151.png" style="" /></td></tr>
</table>

<p>A little bit more complex model than previous one is a tree layer neural network (it is a neural network with two nidden layers):</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/152.png" style="" /></td></tr>
</table>

<p>Even more complex neural network, which we can call <strong>deep neural</strong> network, is for example, a six layer neural network (or neural network with five hidden layers):</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/153.png" style="" /></td></tr>
</table>

<p>When counting layers in a neural network we count hidden layers as well as the output layer, <strong>but we donâ€™t count an input layer</strong>.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/154.png" style="" /></td></tr>
</table>

<p>Here is the notation overview that we will use to describe deep neural networks:</p>

<p>Here is a four layer neural network, so it is a neural network with three hidden layers. Notation we will use for this neural network is:</p>

<ul>
  <li>ğ¿ to denote the number of layers in a neural network
    <ul>
      <li>in this neural network ğ¿=4</li>
    </ul>
  </li>
  <li>ğ‘›<sup>[ğ‘™]</sup> to denote a number of layers in the ğ‘™<sup>ğ‘¡â„</sup> layer
    <ul>
      <li>ğ‘›<sup>[1]</sup>=4, there are four units in the first layer</li>
      <li>ğ‘›<sup>[2]</sup>=4, there are four units in the second layer</li>
      <li>ğ‘›<sup>[3]</sup>=3, there are three units in the thirs layer</li>
      <li>ğ‘›<sup>[4]</sup>=1, this neural network outputs a scalar value</li>
      <li>ğ‘›<sup>[0]</sup>=ğ‘›<sub>ğ‘¥</sub>=3 because input vector, feature vector, has three features</li>
    </ul>
  </li>
  <li>ğ‘<sup>[ğ‘™]</sup>=ğ‘”(ğ‘§<sup>[ğ‘™]</sup>) to denote activation functions in the ğ‘™<sup>ğ‘¡â„</sup> layer
    <ul>
      <li>ğ‘¥=ğ‘<sup>[0]</sup></li>
    </ul>
  </li>
  <li>ğ–<sup>[ğ‘™]</sup> to denote weights for computing ğ‘§<sup>[ğ‘™]</sup></li>
</ul>

<h3 id="forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</h3>

<p>Once again we will see how the forward propagation equations look like. We will show equation for the neural network ilustrated above. In addition, below every two equations we will show the dimensions of vectors or matrices used in the calculations.</p>

<p>A vectorized version of these equations, equations considering all input examples, and correspodnding dimensions of these matrices (which are printed in gray as above) are:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/155.png" style="" /></td></tr>
</table>

<p>From equations we have written, we can see that generalized equations for layer ğ‘™:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/156.png" style="" /></td></tr>
</table>

<p>In case that you are thinking how can we add ğ‘<sup>[ğ‘™]</sup></p>

<p>Notice that, when making a calculation for the first layer, we can also write ğ‘§<sup>[1]</sup>=ğ–<sup>[1]</sup>ğ‘<sup>[0]</sup>+ğ‘<sup>[1]</sup>. So, instead of using ğ‘¥ we use ğ‘<sup>[0]</sup> as an activations in the input layer. ğ‘”<sup>[1]</sup> is activation function in the first layer. Remember that we can choose different activation functions in a Neural Network, but in a single layer we must use the same activation function, so in the output layer we have ğ‘”<sup>[2]</sup> as the activation function and so on.</p>

<p>Matrix YÌ‚ is a matrix of predictions for all input examples, so it is the output of a neural network when the input is matrix ğ—, matrix of all input examples (or a feature matrix).</p>

<p>We can see that there must be a ğ‘“ğ‘œğ‘Ÿ loop, going through all layers in a neural network and calculating all ğ‘<sup>[ğ‘™]</sup> and ğ´<sup>[ğ‘™]</sup> values (where ğ‘™ is a number of the layer). Here, it is prefectly fine to use an explicit for loop.</p>

<h3 id="getting-your-matrix-dimensions-right">Getting your matrix dimensions right</h3>

<p>forward propagation matrix dimensions check</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/158.png" style="" /></td></tr>
</table>

<p>From equations we have written, we can see that generalized equations for layer ğ‘™:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/159.png" style="" /></td></tr>
</table>

<h3 id="why-deep-representations">Why deep representations?</h3>

<p>Weâ€™ve heard that neural networks work really well for a lot of problems. However, neural networks doesnâ€™t need only to be big. Neural Networks also need to be deep or to have a lot hidden layers.</p>

<p>If we are, for example, building a system for an image classification, here is what a deep neural network could be computing. The input of a neural network is a picture of a face. The first layer of the neural network could be a feature detector, or an edge detector. So, the first layer can look at the pictures and find out where are the edges in the picture. Then, in next layer those detected edges could be grouped together to form parts of faces. By putting a lot of edges it can start to detect different parts of faces. For example, we might have a low neurons trying to see if itâ€™s finding an eye or a different neuron trying to find  part of a nose. Finally, putting together eyes, nose etc. it can recognise different faces.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/170.png" style="" /></td></tr>
</table>

<p>To conclude, earlier layers of a neural network detects simpler functions (like edges), and composing them together, in the later layers of a neural network, deep neural network can compute more complex functions.</p>

<p>In case of trying to build a speech recognition system, the first layer could detect if a tone is going up or down or is it a white noise or a slithering sound or some other low level wave of features. In the following layer by composing low level wave forms, nural network might be abe to learn to detect basic units of sound â€“ phonems. In the word cat phonemes are c, a and t. Composing all this together a deep neural network might be able to recognize words and maybe sentences.</p>

<p>So the general intuition behind everything we have said is that earlier layers learn lower level simple features and then later deep layers put together the simpler things it has detected in order to detect more complex things, so that a deep neural network can do some really complex things.</p>

<h3 id="building-blocks-of-deep-neural-networks">Building blocks of deep neural networks</h3>

<p>Letâ€™s see what are the building blocks of a Deep Neural Network.</p>

<p>We will pick one layer, for example layer ğ‘™ of a deep neural network and we will focus on computatons for that layer. For layer ğ‘™ we have parameters ğ–<sup>[ğ‘™]</sup> and ğ‘<sup>[ğ‘™]</sup>. Calculation of the forward pass for layer ğ‘™ we get as we input activations from the previous layer and as the output we get  activations of the current layer, layer ğ‘™.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/171.png" style="" /></td></tr>
</table>

<p>Equations for this calculation step are:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/172.png" style="" /></td></tr>
</table>

<p>where ğ‘”(ğ‘§<sup>[ğ‘™]</sup>) is an activation function in the layer ğ‘™.</p>

<p>It is good to cache the value of ğ‘§<sup>[ğ‘™]</sup> for calculations in backwardpass.</p>

<p>Backward pass is done as we input ğ‘‘ğ‘<sup>[ğ‘™]</sup> and we get the output ğ‘‘ğ‘<sup>[ğ‘™âˆ’1]</sup>, as presented in the following graph. We will always draw backward passes in red.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/173.png" style="" /></td></tr>
</table>

<p>In the following picture we can see a diagram of both a forward and a backward pass in the layer ğ‘™. So, to calulate values in the backward pass we need cached values. Here we just draw ğ‘§<sup>[ğ‘™]</sup> as a cached value, but indeed we will need to cache also ğ‘Š<sup>[ğ‘™]</sup> and ğ‘<sup>[ğ‘™]</sup>.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/174.png" style="" /></td></tr>
</table>

<p>If we implement these two calculations as presented in a graph above, the computation for an ğ¿ layer neural network will be as follows. We will get ğ‘[0], which is our feature vector, feed it in, and that will compute the activations of the first layer. The same thing we will do with next layers.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/08.png" style="" /></td></tr>
</table>

<p>Having all derivative terms we can update parameters:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/175.png" style="" /></td></tr>
</table>

<p>In our programming implementation of this algorithm, when we cache ğ‘§<sup>[ğ‘™]</sup> for backpropagation calculations we will cache also ğ–<sup>[ğ‘™]</sup> and ğ‘<sup>[ğ‘™]</sup>, and ğ‘<sup>[ğ‘™âˆ’1]</sup>.</p>

<h3 id="forward-and-backward-propagation">Forward and Backward Propagation</h3>

<p>Here, we will see equations for calculating the forward step.</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/176.png" style="" /></td></tr>
</table>

<p>Here we will see equations for caluculating backward step:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/157.png" style="" /></td></tr>
</table>

<p>Remember that âˆ— represents an element wise multiplication.</p>

<p>A multi layer neural network is presented in the picture below:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/177.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/180.png" style="" /></td></tr>
</table>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/178.png" style="" /></td></tr>
</table>

<p>and for vectorized version we have:</p>

<table class="image">

<tr><td><img src="/doks-theme/assets/images/notes/neural-networks-and-deep-learning/179.png" style="" /></td></tr>
</table>

<h3 id="parameters-vs-hyperparameters">Parameters vs Hyperparameters</h3>

<p>For building a deep neural network it is very important to organize both parameters and hyperparameters. Parameters of a deep neural network are ğ–<sup>[1]</sup>,ğ‘<sup>[1]</sup>,ğ–<sup>[2]</sup>,ğ‘<sup>[2]</sup>,ğ–<sup>[3]</sup>,ğ‘<sup>[3]</sup> â€¦ and deep neural network also has other parameters which are crucial for our algorithm. Those parameters are :</p>

<ul>
  <li>a learning rate ğ›¼</li>
  <li>a number of iteration</li>
  <li>a number of layers ğ¿</li>
  <li>a number of hidden units ğ‘›<sup>[1]</sup>,ğ‘›<sup>[2]</sup>,â€¦ğ‘›<sup>[ğ¿]</sup></li>
  <li>a choice of activation function</li>
</ul>

<p>These are parameters that control our parameters ğ–<sup>[1]</sup>,ğ‘<sup>[1]</sup>,ğ–<sup>[2]</sup>,ğ‘<sup>[2]</sup>,ğ–<sup>[3]</sup>,ğ‘<sup>[3]</sup> â€¦ and we call them <strong>hyperparameters</strong>. In deep learning there are also these parameters: momentum, bach size, number of epochs etc â€¦</p>

<p>We can see that <strong>hyperparameters</strong> are the variables that determines the network structure and the variables which determine how the network will be trained. Notice also that <strong>hyperparameters</strong> are set before training or before optimizing weights and biases.</p>

<p>To conclude, model parameters are estimated from data during the training step and model hyperparameters are manually set earlier and are used  to assist estimation model parameter.</p>

						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
					
						<!-- <hr> -->
						<p class="site-footer__copyright">Copyright &copy; 2020. - Pedro Abundio Wang <br>All rights reserved.</p>
					
				</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>

		</div><!-- /.js-footer-area -->
	</body>
</html>
